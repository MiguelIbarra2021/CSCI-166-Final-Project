{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtOuIKifmmyk"
      },
      "source": [
        "# Test 3 --- Final Test for 06.11.2025"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqL9333gml9l",
        "outputId": "70c32a3d-216c-4e64-ebaf-1301f39612b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: tensorboard in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (2.20.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from tensorboard) (2.3.1)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from tensorboard) (1.76.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from tensorboard) (3.10)\n",
            "Requirement already satisfied: numpy>=1.12.0 in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from tensorboard) (2.2.6)\n",
            "Requirement already satisfied: packaging in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from tensorboard) (25.0)\n",
            "Requirement already satisfied: pillow in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from tensorboard) (12.0.0)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from tensorboard) (6.33.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from tensorboard) (80.9.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from tensorboard) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from tensorboard) (3.1.3)\n",
            "Requirement already satisfied: typing-extensions~=4.12 in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from grpcio>=1.48.2->tensorboard) (4.15.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from werkzeug>=1.0.1->tensorboard) (3.0.3)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.2 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: tabulate in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (0.9.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.2 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: gymnasium[accept-rom-license,atari] in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from gymnasium[accept-rom-license,atari]) (2.2.6)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from gymnasium[accept-rom-license,atari]) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from gymnasium[accept-rom-license,atari]) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from gymnasium[accept-rom-license,atari]) (0.0.4)\n",
            "Requirement already satisfied: ale_py>=0.9 in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from gymnasium[accept-rom-license,atari]) (0.11.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: gymnasium 1.2.2 does not provide the extra 'accept-rom-license'\n",
            "\n",
            "[notice] A new release of pip is available: 25.2 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: autorom in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (0.6.1)\n",
            "Requirement already satisfied: click in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from autorom) (8.3.0)\n",
            "Requirement already satisfied: requests in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from autorom) (2.32.5)\n",
            "Requirement already satisfied: colorama in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from click->autorom) (0.4.6)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from requests->autorom) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from requests->autorom) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from requests->autorom) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from requests->autorom) (2025.10.5)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.2 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: stable-baselines3 in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (2.7.0)\n",
            "Requirement already satisfied: gymnasium<1.3.0,>=0.29.1 in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from stable-baselines3) (1.2.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.20 in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from stable-baselines3) (2.2.6)\n",
            "Requirement already satisfied: torch<3.0,>=2.3 in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from stable-baselines3) (2.9.0)\n",
            "Requirement already satisfied: cloudpickle in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from stable-baselines3) (3.1.2)\n",
            "Requirement already satisfied: pandas in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from stable-baselines3) (2.3.3)\n",
            "Requirement already satisfied: matplotlib in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from stable-baselines3) (3.10.7)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from gymnasium<1.3.0,>=0.29.1->stable-baselines3) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from gymnasium<1.3.0,>=0.29.1->stable-baselines3) (0.0.4)\n",
            "Requirement already satisfied: filelock in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from torch<3.0,>=2.3->stable-baselines3) (3.20.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from torch<3.0,>=2.3->stable-baselines3) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from torch<3.0,>=2.3->stable-baselines3) (3.5)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from torch<3.0,>=2.3->stable-baselines3) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from torch<3.0,>=2.3->stable-baselines3) (2025.10.0)\n",
            "Requirement already satisfied: setuptools in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from torch<3.0,>=2.3->stable-baselines3) (80.9.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from sympy>=1.13.3->torch<3.0,>=2.3->stable-baselines3) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from jinja2->torch<3.0,>=2.3->stable-baselines3) (3.0.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib->stable-baselines3) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib->stable-baselines3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib->stable-baselines3) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib->stable-baselines3) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib->stable-baselines3) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib->stable-baselines3) (12.0.0)\n",
            "Requirement already satisfied: pyparsing>=3 in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib->stable-baselines3) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib->stable-baselines3) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3) (1.17.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from pandas->stable-baselines3) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from pandas->stable-baselines3) (2025.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.2 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: imageio[ffmpeg] in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (2.37.2)\n",
            "Requirement already satisfied: numpy in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from imageio[ffmpeg]) (2.2.6)\n",
            "Requirement already satisfied: pillow>=8.3.2 in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from imageio[ffmpeg]) (12.0.0)\n",
            "Collecting imageio-ffmpeg (from imageio[ffmpeg])\n",
            "  Downloading imageio_ffmpeg-0.6.0-py3-none-win_amd64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: psutil in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from imageio[ffmpeg]) (7.1.3)\n",
            "Downloading imageio_ffmpeg-0.6.0-py3-none-win_amd64.whl (31.2 MB)\n",
            "   ---------------------------------------- 0.0/31.2 MB ? eta -:--:--\n",
            "   -------- ------------------------------- 6.8/31.2 MB 37.2 MB/s eta 0:00:01\n",
            "   ------------------ --------------------- 14.4/31.2 MB 36.3 MB/s eta 0:00:01\n",
            "   ---------------------------- ----------- 22.0/31.2 MB 35.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  30.7/31.2 MB 37.1 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 31.2/31.2 MB 34.0 MB/s  0:00:00\n",
            "Installing collected packages: imageio-ffmpeg\n",
            "Successfully installed imageio-ffmpeg-0.6.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.2 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "# Install TensorBoard for visualization\n",
        "%pip install tensorboard\n",
        "%pip install tabulate\n",
        "\n",
        "# Install required packages for reinforcement learning with Atari environments\n",
        "%pip install gymnasium[atari,accept-rom-license]\n",
        "%pip install autorom\n",
        "%pip install stable-baselines3\n",
        "\n",
        "# Install imageio for video recording\n",
        "%pip install imageio[ffmpeg]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oi2qmVoVmxve",
        "outputId": "8d9a366d-9bc7-4ae0-dfaf-2c73db288b7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AutoROM will download the Atari 2600 ROMs.\n",
            "They will be installed to:\n",
            "\tC:\\Users\\migue\\AppData\\Roaming\\Python\\Python313\\site-packages\\AutoROM\\roms\n",
            "\n",
            "Existing ROMs will be overwritten.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\migue\\AppData\\Roaming\\Python\\Python313\\site-packages\\AutoROM\\AutoROM.py:264: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
            "  import pkg_resources\n"
          ]
        }
      ],
      "source": [
        "# Download Atari ROMs\n",
        "!AutoROM --accept-license"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQyqgMFzT-qi"
      },
      "source": [
        "# Install the Gym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "_W_afhrzUAnp"
      },
      "outputs": [],
      "source": [
        "import ale_py\n",
        "import gymnasium as gym"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0n7zQrALq7M"
      },
      "source": [
        "# Configure the model save folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "vlWPUjKfLv9y"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "save_dir = \"models/\"\n",
        "os.makedirs(save_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAEvyoukL1T3"
      },
      "source": [
        "# Now Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "VCbjkLLSxCUN"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "import argparse\n",
        "import time\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import collections\n",
        "import typing as tt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.utils.tensorboard.writer import SummaryWriter\n",
        "\n",
        "import imageio\n",
        "from IPython.display import HTML, display\n",
        "from base64 import b64encode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "dIJ32Rs6xJsV"
      },
      "outputs": [],
      "source": [
        "#dqn_model\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, input_shape, n_actions):\n",
        "        super(DQN, self).__init__()\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "        )\n",
        "        size = self.conv(torch.zeros(1, *input_shape)).size()[-1]\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, n_actions)\n",
        "        )\n",
        "    def forward(self, x: torch.ByteTensor):\n",
        "        x = x.float() / 255.0\n",
        "        return self.fc(self.conv(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vk2CtGcOBcQo",
        "outputId": "84795c85-6ace-4c26-90ea-9b73fc4766d3"
      },
      "outputs": [],
      "source": [
        "#wrappers\n",
        "\n",
        "from gymnasium import spaces\n",
        "from stable_baselines3.common import atari_wrappers\n",
        "\n",
        "\n",
        "class ImageToPyTorch(gym.ObservationWrapper):\n",
        "    \"\"\"\n",
        "    ImageToPyTorch: Reorders image dimensions from (H, W, C) to (C, H, W)\n",
        "    for compatibility with PyTorch convolutional layers.\n",
        "    \"\"\"\n",
        "    def __init__(self, env):\n",
        "        super(ImageToPyTorch, self).__init__(env)\n",
        "        obs = self.observation_space\n",
        "        assert isinstance(obs, gym.spaces.Box)\n",
        "        assert len(obs.shape) == 3\n",
        "        new_shape = (obs.shape[-1], obs.shape[0], obs.shape[1])\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=obs.low.min(), high=obs.high.max(),\n",
        "            shape=new_shape, dtype=obs.dtype)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        return np.moveaxis(observation, 2, 0)\n",
        "\n",
        "\n",
        "class BufferWrapper(gym.ObservationWrapper):\n",
        "    \"\"\"\n",
        "    BufferWrapper: Maintains a rolling window of the last `n_steps` frames\n",
        "    to give the agent a sense of temporal context.\n",
        "    \"\"\"\n",
        "    def __init__(self, env, n_steps):\n",
        "        super(BufferWrapper, self).__init__(env)\n",
        "        obs = env.observation_space\n",
        "        assert isinstance(obs, spaces.Box)\n",
        "        new_obs = gym.spaces.Box(\n",
        "            obs.low.repeat(n_steps, axis=0), obs.high.repeat(n_steps, axis=0),\n",
        "            dtype=obs.dtype)\n",
        "        self.observation_space = new_obs\n",
        "        self.buffer = collections.deque(maxlen=n_steps)\n",
        "\n",
        "    def reset(self, *, seed: tt.Optional[int] = None, options: tt.Optional[dict[str, tt.Any]] = None):\n",
        "        for _ in range(self.buffer.maxlen):\n",
        "            self.buffer.append(np.zeros_like(self.env.observation_space.low))\n",
        "        obs, extra = self.env.reset()\n",
        "        return self.observation(obs), extra\n",
        "\n",
        "    def observation(self, observation: np.ndarray) -> np.ndarray:\n",
        "        self.buffer.append(observation)\n",
        "        return np.concatenate(self.buffer)\n",
        "\n",
        "\n",
        "def make_env(env_name: str, n_steps=4, render_mode=None, **kwargs):\n",
        "    print(f\"Creating environment {env_name}\")\n",
        "    env = gym.make(env_name, render_mode=render_mode, **kwargs)\n",
        "    env = atari_wrappers.AtariWrapper(env, clip_reward=False, noop_max=0)\n",
        "    env = ImageToPyTorch(env)\n",
        "    env = BufferWrapper(env, n_steps=n_steps)\n",
        "    return env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Experiment logging utility\n",
        "from IPython.display import Markdown, clear_output, display\n",
        "import pandas as pd\n",
        "import datetime\n",
        "\n",
        "# Persistent experiment history\n",
        "config_history = []\n",
        "version_counter = 0\n",
        "\n",
        "# Log experiment configurations\n",
        "def log_experiment(config_dict, note=\"\"):\n",
        "    \"\"\"Append config to table and display nicely.\"\"\"\n",
        "    global version_counter, config_history\n",
        "    entry = {\n",
        "        \"version\": version_counter,\n",
        "        \"timestamp\": datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "        **config_dict,\n",
        "        \"note\": note\n",
        "    }\n",
        "    # Append the new entry to the history\n",
        "    config_history.append(entry)\n",
        "    version_counter += 1\n",
        "\n",
        "    # Display the updated experiment log\n",
        "    clear_output(wait=True)\n",
        "    df = pd.DataFrame(config_history)\n",
        "    display(Markdown(df.to_markdown(index=False)))\n",
        "\n",
        "def remove_last_entry():\n",
        "    \"\"\"Remove the last entry from the experiment log.\"\"\"\n",
        "    global config_history, version_counter\n",
        "    if config_history:\n",
        "        config_history.pop()\n",
        "        version_counter -= 1\n",
        "        clear_output(wait=True)\n",
        "        df = pd.DataFrame(config_history)\n",
        "        display(Markdown(df.to_markdown(index=False)))\n",
        "\n",
        "def clear_table():\n",
        "    \"\"\"Clear the experiment log table.\"\"\"\n",
        "    global config_history, version_counter\n",
        "    config_history = []\n",
        "    version_counter = 0\n",
        "    clear_output(wait=True)\n",
        "\n",
        "def print_table_for_github():\n",
        "    \"\"\"Print the experiment log table in markdown format for GitHub.\"\"\"\n",
        "    df = pd.DataFrame(config_history)\n",
        "    print(df.to_markdown(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Removes the last entry entered; only if I mean to\n",
        "can_remove = False\n",
        "\n",
        "if can_remove:\n",
        "    remove_last_entry()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clears the whole table, only if I mean to\n",
        "can_clear = False\n",
        "\n",
        "if can_clear:\n",
        "    clear_table()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "|   version | timestamp           | DEFAULT_ENV_NAME   |   MEAN_REWARD_BOUND |   GAMMA |   BATCH_SIZE |   REPLAY_SIZE |   LEARNING_RATE |   SYNC_TARGET_FRAMES |   REPLAY_START_SIZE |   SAVE_EPSILON |   EPSILON_DECAY_LAST_FRAME |   EPSILON_START |   EPSILON_FINAL | note         |\n",
              "|----------:|:--------------------|:-------------------|--------------------:|--------:|-------------:|--------------:|----------------:|---------------------:|--------------------:|---------------:|---------------------------:|----------------:|----------------:|:-------------|\n",
              "|         0 | 2025-11-12 08:13:05 | ALE/Pong-v5        |                  19 |    0.99 |           32 |         10000 |          0.0001 |                 1000 |               10000 |            0.5 |                     150000 |               1 |            0.01 | Baseline run |\n",
              "|         1 | 2025-11-12 08:13:09 | ALE/Pong-v5        |                   5 |    0.99 |           32 |         10000 |          0.0001 |                  500 |                1000 |            0.5 |                      10000 |               1 |            0.01 | Quick run    |"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Prints the table in markdown format\n",
        "print_table_markdown()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "id": "GvXPdjPCBxOd"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "|   version | timestamp           | DEFAULT_ENV_NAME   |   MEAN_REWARD_BOUND |   GAMMA |   BATCH_SIZE |   REPLAY_SIZE |   LEARNING_RATE |   SYNC_TARGET_FRAMES |   REPLAY_START_SIZE |   SAVE_EPSILON |   EPSILON_DECAY_LAST_FRAME |   EPSILON_START |   EPSILON_FINAL | note         |\n",
              "|----------:|:--------------------|:-------------------|--------------------:|--------:|-------------:|--------------:|----------------:|---------------------:|--------------------:|---------------:|---------------------------:|----------------:|----------------:|:-------------|\n",
              "|         0 | 2025-11-12 08:13:05 | ALE/Pong-v5        |                  19 |    0.99 |           32 |         10000 |          0.0001 |                 1000 |               10000 |            0.5 |                     150000 |               1 |            0.01 | Baseline run |"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Base Configuration\n",
        "config = {\n",
        "    \"DEFAULT_ENV_NAME\": \"ALE/Pong-v5\",\n",
        "    \"MEAN_REWARD_BOUND\": 19,\n",
        "\n",
        "    \"GAMMA\": 0.99,\n",
        "    \"BATCH_SIZE\": 32,\n",
        "    \"REPLAY_SIZE\": 10000,\n",
        "    \"LEARNING_RATE\": 1e-4,\n",
        "    \"SYNC_TARGET_FRAMES\": 1000,\n",
        "    \"REPLAY_START_SIZE\": 10000,\n",
        "\n",
        "    \"SAVE_EPSILON\": 0.5,  # Only save if at least this much better\n",
        "    \"EPSILON_DECAY_LAST_FRAME\": 150000,\n",
        "    \"EPSILON_START\": 1.0,\n",
        "    \"EPSILON_FINAL\": 0.01\n",
        "}\n",
        "\n",
        "# Log automatically when you rerun this cell\n",
        "log_experiment(config, note=\"Baseline run\")\n",
        "\n",
        "# Tuple of tensors returned from a sampled minibatch in replay buffer\n",
        "State = np.ndarray\n",
        "Action = int\n",
        "BatchTensors = tt.Tuple[\n",
        "    torch.ByteTensor,           # current state\n",
        "    torch.LongTensor,           # actions\n",
        "    torch.Tensor,               # rewards\n",
        "    torch.BoolTensor,           # done || trunc\n",
        "    torch.ByteTensor            # next state\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "id": "3FHvHNMrR9Sk"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "|   version | timestamp           | DEFAULT_ENV_NAME   |   MEAN_REWARD_BOUND |   GAMMA |   BATCH_SIZE |   REPLAY_SIZE |   LEARNING_RATE |   SYNC_TARGET_FRAMES |   REPLAY_START_SIZE |   SAVE_EPSILON |   EPSILON_DECAY_LAST_FRAME |   EPSILON_START |   EPSILON_FINAL | note         |\n",
              "|----------:|:--------------------|:-------------------|--------------------:|--------:|-------------:|--------------:|----------------:|---------------------:|--------------------:|---------------:|---------------------------:|----------------:|----------------:|:-------------|\n",
              "|         0 | 2025-11-12 08:13:05 | ALE/Pong-v5        |                  19 |    0.99 |           32 |         10000 |          0.0001 |                 1000 |               10000 |            0.5 |                     150000 |               1 |            0.01 | Baseline run |\n",
              "|         1 | 2025-11-12 08:13:09 | ALE/Pong-v5        |                   5 |    0.99 |           32 |         10000 |          0.0001 |                  500 |                1000 |            0.5 |                      10000 |               1 |            0.01 | Quick run    |"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# âš™ï¸ Fast Training Config for Quick Test Run\n",
        "config[\"MEAN_REWARD_BOUND\"] = 5\n",
        "config[\"REPLAY_START_SIZE\"] = 1000\n",
        "config[\"EPSILON_DECAY_LAST_FRAME\"] = 10000\n",
        "config[\"SYNC_TARGET_FRAMES\"] = 500\n",
        "\n",
        "# config[\"REPLAY_SIZE\"] = 5000  # optional\n",
        "# config[\"BATCH_SIZE\"] = 16     # optional\n",
        "\n",
        "# Log automatically when you rerun this cell\n",
        "log_experiment(config, note=\"Quick run\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "MkILCuT2OhBV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Define directories\n",
        "save_dir_local = \"saved_models\"\n",
        "\n",
        "# Create both directories if they don't exist\n",
        "os.makedirs(save_dir_local, exist_ok=True)\n",
        "\n",
        "# Safe model filename\n",
        "env_name = config[\"DEFAULT_ENV_NAME\"]\n",
        "safe_env_name = env_name.replace(\"/\", \"_\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "uW4Bo-mcB6rc"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class Experience:\n",
        "    state: State\n",
        "    action: Action\n",
        "    reward: float\n",
        "    done_trunc: bool\n",
        "    new_state: State\n",
        "\n",
        "\n",
        "class ExperienceBuffer:\n",
        "    def __init__(self, capacity: int):\n",
        "        self.buffer = collections.deque(maxlen=capacity)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def append(self, experience: Experience):\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    def sample(self, batch_size: int) -> tt.List[Experience]:\n",
        "        indices = np.random.choice(len(self), batch_size, replace=False)\n",
        "        return [self.buffer[idx] for idx in indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "irJb4V32B-R8"
      },
      "outputs": [],
      "source": [
        "class Agent:\n",
        "    def __init__(self, env: gym.Env, exp_buffer: ExperienceBuffer):\n",
        "        self.env = env\n",
        "        self.exp_buffer = exp_buffer\n",
        "        self.state: tt.Optional[np.ndarray] = None\n",
        "        self._reset()\n",
        "\n",
        "    def _reset(self):\n",
        "        self.state, _ = self.env.reset()\n",
        "        self.total_reward = 0.0\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def play_step(self, net: DQN, device: torch.device,\n",
        "                  epsilon: float = 0.0) -> tt.Optional[float]:\n",
        "        done_reward = None\n",
        "\n",
        "        if np.random.random() < epsilon:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            state_v = torch.as_tensor(self.state).to(device)\n",
        "            state_v.unsqueeze_(0)\n",
        "            q_vals_v = net(state_v)\n",
        "            _, act_v = torch.max(q_vals_v, dim=1)\n",
        "            action = int(act_v.item())\n",
        "\n",
        "        # do step in the environment\n",
        "        new_state, reward, is_done, is_tr, _ = self.env.step(action)\n",
        "        self.total_reward += reward\n",
        "\n",
        "        exp = Experience(\n",
        "            state=self.state, action=action, reward=float(reward),\n",
        "            done_trunc=is_done or is_tr, new_state=new_state\n",
        "        )\n",
        "        self.exp_buffer.append(exp)\n",
        "        self.state = new_state\n",
        "        if is_done or is_tr:\n",
        "            done_reward = self.total_reward\n",
        "            self._reset()\n",
        "        return done_reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "vHXmNr_wCBJm"
      },
      "outputs": [],
      "source": [
        "def batch_to_tensors(batch: tt.List[Experience], device: torch.device) -> BatchTensors:\n",
        "    states, actions, rewards, dones, new_state = [], [], [], [], []\n",
        "    for e in batch:\n",
        "        states.append(e.state)\n",
        "        actions.append(e.action)\n",
        "        rewards.append(e.reward)\n",
        "        dones.append(e.done_trunc)\n",
        "        new_state.append(e.new_state)\n",
        "    states_t = torch.as_tensor(np.asarray(states))\n",
        "    actions_t = torch.LongTensor(actions)\n",
        "    rewards_t = torch.FloatTensor(rewards)\n",
        "    dones_t = torch.BoolTensor(dones)\n",
        "    new_states_t = torch.as_tensor(np.asarray(new_state))\n",
        "    return states_t.to(device), actions_t.to(device), rewards_t.to(device), \\\n",
        "           dones_t.to(device),  new_states_t.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "-dbh0431CEXO"
      },
      "outputs": [],
      "source": [
        "def calc_loss(batch: tt.List[Experience], net: DQN, tgt_net: DQN,\n",
        "              device: torch.device) -> torch.Tensor:\n",
        "    states_t, actions_t, rewards_t, dones_t, new_states_t = batch_to_tensors(batch, device)\n",
        "\n",
        "    state_action_values = net(states_t).gather(\n",
        "        1, actions_t.unsqueeze(-1)\n",
        "    ).squeeze(-1)\n",
        "    # DQN:\n",
        "    with torch.no_grad():\n",
        "        next_state_values = tgt_net(new_states_t).max(1)[0]\n",
        "        next_state_values[dones_t] = 0.0\n",
        "        next_state_values = next_state_values.detach()\n",
        "\n",
        "    # Double DQN:\n",
        "    #with torch.no_grad():\n",
        "    #    next_actions = net(new_states_t).argmax(dim=1)  # choose using online net\n",
        "    #    next_state_values = tgt_net(new_states_t).gather(1, next_actions.unsqueeze(-1)).squeeze(-1)\n",
        "    #    next_state_values[dones_t] = 0.0\n",
        "\n",
        "    expected_state_action_values = rewards_t + config[\"GAMMA\"] * next_state_values; \n",
        "\n",
        "    return nn.MSELoss()(state_action_values, expected_state_action_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def record_policy_video(env_name, model, device, video_filename=\"policy_rollout.mp4\", max_steps=2000):\n",
        "    \"\"\"Run one greedy episode using the given model and save a video.\"\"\"\n",
        "    # Use your make_env to preserve preprocessing\n",
        "    video_env = make_env(env_name, render_mode=\"rgb_array\")\n",
        "    state, _ = video_env.reset(seed=42)\n",
        "    frames, total_reward = [], 0.0\n",
        "\n",
        "    for step in range(max_steps):\n",
        "        # Capture a rendered RGB frame (already handled by render_mode)\n",
        "        frame = video_env.render()\n",
        "        if isinstance(frame, np.ndarray):\n",
        "            frames.append(frame)\n",
        "\n",
        "        # Greedy action\n",
        "        state_v = torch.as_tensor(state).unsqueeze(0).to(device)\n",
        "        q_vals_v = model(state_v)\n",
        "        _, act_v = torch.max(q_vals_v, dim=1)\n",
        "        action = int(act_v.item())\n",
        "\n",
        "        state, reward, terminated, truncated, _ = video_env.step(action)\n",
        "        total_reward += reward\n",
        "        if terminated or truncated:\n",
        "            frames.append(video_env.render())\n",
        "            break\n",
        "\n",
        "    video_env.close()\n",
        "\n",
        "    # Save video\n",
        "    if frames:\n",
        "        os.makedirs(os.path.dirname(video_filename), exist_ok=True)\n",
        "        with imageio.get_writer(video_filename, fps=30) as writer:\n",
        "            for f in frames:\n",
        "                writer.append_data(f)\n",
        "        print(f\"ðŸŽ¬ Saved gameplay video: {video_filename} | Total reward: {total_reward:.2f}\")\n",
        "    else:\n",
        "        print(\"âš ï¸ No frames captured â€” render_mode may not be supported.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8gj-5woCXEB",
        "outputId": "afd1308e-a016-4b02-b944-4321357467fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n",
            "Creating environment ALE/Pong-v5\n",
            "DQN(\n",
            "  (conv): Sequential(\n",
            "    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
            "    (1): ReLU()\n",
            "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
            "    (3): ReLU()\n",
            "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (5): ReLU()\n",
            "    (6): Flatten(start_dim=1, end_dim=-1)\n",
            "  )\n",
            "  (fc): Sequential(\n",
            "    (0): Linear(in_features=3136, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=6, bias=True)\n",
            "  )\n",
            ")\n",
            "249: done 1 games, reward -21.000, eps 0.98, speed 293.40 f/s, time 0.0 min\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-21-20251110-1137-test_epsdec10000_rs1000_sync500.dat\n",
            "Creating environment ALE/Pong-v5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸŽ¬ Saved gameplay video: videos/ALE_Pong-v5--21-20251110-1137.mp4 | Total reward: -21.00\n",
            "2895: done 13 games, reward -20.462, eps 0.71, speed 27.86 f/s, time 1.3 min\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-20-20251110-1138-test_epsdec10000_rs1000_sync500.dat\n",
            "Best reward updated -21.000 -> -20.462\n",
            "Creating environment ALE/Pong-v5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸŽ¬ Saved gameplay video: videos/ALE_Pong-v5--20-20251110-1138.mp4 | Total reward: -21.00\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[125]\u001b[39m\u001b[32m, line 72\u001b[39m\n\u001b[32m     70\u001b[39m     batch = buffer.sample(config[\u001b[33m\"\u001b[39m\u001b[33mBATCH_SIZE\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     71\u001b[39m     loss_t = calc_loss(batch, net, tgt_net, device)\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m     \u001b[43mloss_t\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     73\u001b[39m     optimizer.step()\n\u001b[32m     74\u001b[39m env.close()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\autograd\\__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\autograd\\graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "model_comment = f\"test_epsdec{config[\"EPSILON_DECAY_LAST_FRAME\"]}_rs{config[\"REPLAY_START_SIZE\"]}_sync{config[\"SYNC_TARGET_FRAMES\"]}\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "env = make_env(env_name)\n",
        "net = DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
        "tgt_net = DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
        "writer = SummaryWriter(comment=f\"-{env_name}-{model_comment}\")\n",
        "print(net)\n",
        "\n",
        "buffer = ExperienceBuffer(config[\"REPLAY_SIZE\"])\n",
        "agent = Agent(env, buffer)\n",
        "epsilon = config[\"EPSILON_START\"]\n",
        "\n",
        "optimizer = optim.Adam(net.parameters(), lr=config[\"LEARNING_RATE\"])\n",
        "total_rewards = []\n",
        "frame_idx = 0\n",
        "ts_frame = 0\n",
        "ts = time.time()\n",
        "best_m_reward = None\n",
        "\n",
        "start_time = time.time()\n",
        "while True:\n",
        "    frame_idx += 1\n",
        "    epsilon = max(config[\"EPSILON_FINAL\"], config[\"EPSILON_START\"] - frame_idx / config[\"EPSILON_DECAY_LAST_FRAME\"])\n",
        "\n",
        "    reward = agent.play_step(net, device, epsilon)\n",
        "    if reward is not None:\n",
        "        total_rewards.append(reward)\n",
        "        speed = (frame_idx - ts_frame) / (time.time() - ts)\n",
        "        elapsed = time.time() - start_time  # in seconds\n",
        "        ts_frame = frame_idx\n",
        "        ts = time.time()\n",
        "        m_reward = np.mean(total_rewards[-100:])\n",
        "        #  print(f\"{frame_idx}: done {len(total_rewards)} games, reward {m_reward:.3f}, \"\n",
        "        #      f\"eps {epsilon:.2f}, speed {speed:.2f} f/s, time {elapsed/60:.1f} min\")\n",
        "        writer.add_scalar(\"epsilon\", epsilon, frame_idx)\n",
        "        writer.add_scalar(\"speed\", speed, frame_idx)\n",
        "        writer.add_scalar(\"reward_100\", m_reward, frame_idx)\n",
        "        writer.add_scalar(\"reward\", reward, frame_idx)\n",
        "        if best_m_reward is None or m_reward > best_m_reward + config[\"SAVE_EPSILON\"]:\n",
        "            print(f\"{frame_idx}: done {len(total_rewards)} games, reward {m_reward:.3f}, \"\n",
        "                f\"eps {epsilon:.2f}, speed {speed:.2f} f/s, time {elapsed/60:.1f} min\")\n",
        "            timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
        "            model_filename = f\"{safe_env_name}-best_{int(m_reward)}-{timestamp}-{model_comment}.dat\"\n",
        "\n",
        "            # Save to both paths\n",
        "            model_path_local = os.path.join(save_dir_local, model_filename)\n",
        "\n",
        "            torch.save(net.state_dict(), model_path_local)\n",
        "\n",
        "            print(f\"Model saved to:\\n - Local:        {model_path_local}\")\n",
        "            if best_m_reward is not None:\n",
        "                print(f\"Best reward updated {best_m_reward:.3f} -> {m_reward:.3f}\")\n",
        "            best_m_reward = m_reward\n",
        "\n",
        "            # ðŸŽ¥ Automatically record a short greedy rollout video\n",
        "            video_filename = f\"videos/{safe_env_name}-{int(m_reward)}-{timestamp}.mp4\"\n",
        "            os.makedirs(\"videos\", exist_ok=True)\n",
        "            record_policy_video(env_name, net, device, video_filename)\n",
        "        if m_reward > config[\"MEAN_REWARD_BOUND\"]:\n",
        "            print(\"Solved in %d frames!\" % frame_idx)\n",
        "            break\n",
        "    if len(buffer) < config[\"REPLAY_START_SIZE\"]:\n",
        "        continue\n",
        "    if frame_idx % config[\"SYNC_TARGET_FRAMES\"] == 0:\n",
        "        tgt_net.load_state_dict(net.state_dict())\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    batch = buffer.sample(config[\"BATCH_SIZE\"])\n",
        "    loss_t = calc_loss(batch, net, tgt_net, device)\n",
        "    loss_t.backward()\n",
        "    optimizer.step()\n",
        "env.close()\n",
        "writer.close()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
