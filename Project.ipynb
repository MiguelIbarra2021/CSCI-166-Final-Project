{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtOuIKifmmyk"
      },
      "source": [
        "# Test 3 --- Final Test for 06.11.2025"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqL9333gml9l",
        "outputId": "70c32a3d-216c-4e64-ebaf-1301f39612b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorboard in c:\\users\\piepie\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.13.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in c:\\users\\piepie\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorboard) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in c:\\users\\piepie\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorboard) (1.56.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\piepie\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorboard) (2.22.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in c:\\users\\piepie\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorboard) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\piepie\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorboard) (3.4.3)\n",
            "Requirement already satisfied: numpy>=1.12.0 in c:\\users\\piepie\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorboard) (1.26.4)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in c:\\users\\piepie\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorboard) (4.25.8)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\piepie\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorboard) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\piepie\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorboard) (65.5.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\piepie\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorboard) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\piepie\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorboard) (2.3.6)\n",
            "Requirement already satisfied: wheel>=0.26 in c:\\users\\piepie\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorboard) (0.40.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\piepie\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\piepie\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\piepie\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n",
            "Requirement already satisfied: six>=1.9.0 in c:\\users\\piepie\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard) (1.16.0)\n",
            "Requirement already satisfied: urllib3<2.0 in c:\\users\\piepie\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard) (1.26.16)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\piepie\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\piepie\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\piepie\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\piepie\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard) (2023.5.7)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\piepie\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth<3,>=1.6.3->tensorboard) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\piepie\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard) (3.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\piepie\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.3)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.2 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tabulate in c:\\users\\piepie\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.9.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.2 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ale-py in c:\\users\\piepie\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.11.2)\n",
            "Requirement already satisfied: numpy>1.20 in c:\\users\\piepie\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ale-py) (1.26.4)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.2 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gymnasium[accept-rom-license,atari] in c:\\users\\piepie\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.2.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\piepie\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gymnasium[accept-rom-license,atari]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\piepie\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gymnasium[accept-rom-license,atari]) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\piepie\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gymnasium[accept-rom-license,atari]) (4.13.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\piepie\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gymnasium[accept-rom-license,atari]) (0.0.4)\n",
            "Requirement already satisfied: ale_py>=0.9 in c:\\users\\piepie\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gymnasium[accept-rom-license,atari]) (0.11.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: gymnasium 1.2.1 does not provide the extra 'accept-rom-license'\n",
            "\n",
            "[notice] A new release of pip is available: 25.2 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: autorom in c:\\users\\piepie\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.6.1)\n",
            "Requirement already satisfied: click in c:\\users\\piepie\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from autorom) (8.3.0)\n",
            "Requirement already satisfied: requests in c:\\users\\piepie\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from autorom) (2.32.3)\n",
            "Requirement already satisfied: colorama in c:\\users\\piepie\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from click->autorom) (0.4.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\piepie\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->autorom) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\piepie\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->autorom) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\piepie\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->autorom) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\piepie\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->autorom) (2023.5.7)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.2 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: stable-baselines3 in c:\\users\\piepie\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.7.0)\n",
            "Requirement already satisfied: gymnasium<1.3.0,>=0.29.1 in c:\\users\\piepie\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from stable-baselines3) (1.2.1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.20 in c:\\users\\piepie\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from stable-baselines3) (1.26.4)\n",
            "Requirement already satisfied: torch<3.0,>=2.3 in c:\\users\\piepie\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from stable-baselines3) (2.5.1+cu121)\n",
            "Requirement already satisfied: cloudpickle in c:\\users\\piepie\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from stable-baselines3) (3.1.1)\n",
            "Requirement already satisfied: pandas in c:\\users\\piepie\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from stable-baselines3) (2.2.3)\n",
            "Requirement already satisfied: matplotlib in c:\\users\\piepie\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from stable-baselines3) (3.10.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\piepie\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gymnasium<1.3.0,>=0.29.1->stable-baselines3) (4.13.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\piepie\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gymnasium<1.3.0,>=0.29.1->stable-baselines3) (0.0.4)\n",
            "Requirement already satisfied: filelock in c:\\users\\piepie\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch<3.0,>=2.3->stable-baselines3) (3.18.0)\n",
            "Requirement already satisfied: networkx in c:\\users\\piepie\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch<3.0,>=2.3->stable-baselines3) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\piepie\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch<3.0,>=2.3->stable-baselines3) (3.1.6)\n",
            "Requirement already satisfied: fsspec in c:\\users\\piepie\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch<3.0,>=2.3->stable-baselines3) (2024.12.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in c:\\users\\piepie\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch<3.0,>=2.3->stable-baselines3) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\piepie\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy==1.13.1->torch<3.0,>=2.3->stable-baselines3) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\piepie\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch<3.0,>=2.3->stable-baselines3) (2.1.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\piepie\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib->stable-baselines3) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\piepie\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib->stable-baselines3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\piepie\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib->stable-baselines3) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\piepie\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib->stable-baselines3) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\piepie\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib->stable-baselines3) (23.1)\n",
            "Requirement already satisfied: pillow>=8 in c:\\users\\piepie\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib->stable-baselines3) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\piepie\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib->stable-baselines3) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\piepie\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib->stable-baselines3) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\piepie\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3) (1.16.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\piepie\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->stable-baselines3) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\piepie\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->stable-baselines3) (2025.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.2 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: imageio[ffmpeg] in c:\\users\\piepie\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.37.2)\n",
            "Requirement already satisfied: numpy in c:\\users\\piepie\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from imageio[ffmpeg]) (1.26.4)\n",
            "Requirement already satisfied: pillow>=8.3.2 in c:\\users\\piepie\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from imageio[ffmpeg]) (11.2.1)\n",
            "Requirement already satisfied: imageio-ffmpeg in c:\\users\\piepie\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from imageio[ffmpeg]) (0.6.0)\n",
            "Requirement already satisfied: psutil in c:\\users\\piepie\\appdata\\roaming\\python\\python311\\site-packages (from imageio[ffmpeg]) (7.0.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.2 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "# Install TensorBoard for visualization\n",
        "%pip install tensorboard\n",
        "%pip install tabulate\n",
        "\n",
        "# Install required packages for reinforcement learning with Atari environments\n",
        "%pip install ale-py\n",
        "%pip install gymnasium[atari,accept-rom-license]\n",
        "%pip install autorom\n",
        "%pip install stable-baselines3\n",
        "\n",
        "# Install imageio for video recording\n",
        "%pip install imageio[ffmpeg]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oi2qmVoVmxve",
        "outputId": "8d9a366d-9bc7-4ae0-dfaf-2c73db288b7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AutoROM will download the Atari 2600 ROMs.\n",
            "They will be installed to:\n",
            "\tC:\\Users\\PiePie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\AutoROM\\roms\n",
            "\n",
            "Existing ROMs will be overwritten.\n"
          ]
        }
      ],
      "source": [
        "# Download Atari ROMs\n",
        "!AutoROM --accept-license"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQyqgMFzT-qi"
      },
      "source": [
        "# Install the Gym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_W_afhrzUAnp"
      },
      "outputs": [],
      "source": [
        "import ale_py\n",
        "import gymnasium as gym"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0n7zQrALq7M"
      },
      "source": [
        "# Configure the model save folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "vlWPUjKfLv9y"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "save_dir = \"models/\"\n",
        "os.makedirs(save_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAEvyoukL1T3"
      },
      "source": [
        "# Now Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "VCbjkLLSxCUN"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "import argparse\n",
        "import time\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import collections\n",
        "import typing as tt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.utils.tensorboard.writer import SummaryWriter\n",
        "\n",
        "import imageio\n",
        "from IPython.display import HTML, display\n",
        "from base64 import b64encode\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"imageio\")\n",
        "os.environ[\"IMAGEIO_FFMPEG_LOGLEVEL\"] = \"quiet\"  # or \"quiet\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "dIJ32Rs6xJsV"
      },
      "outputs": [],
      "source": [
        "#dqn_model\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, input_shape, n_actions):\n",
        "        super(DQN, self).__init__()\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "        )\n",
        "        size = self.conv(torch.zeros(1, *input_shape)).size()[-1]\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, n_actions)\n",
        "        )\n",
        "    def forward(self, x: torch.ByteTensor):\n",
        "        x = x.float() / 255.0\n",
        "        return self.fc(self.conv(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vk2CtGcOBcQo",
        "outputId": "84795c85-6ace-4c26-90ea-9b73fc4766d3"
      },
      "outputs": [],
      "source": [
        "#wrappers\n",
        "\n",
        "from gymnasium import spaces\n",
        "from stable_baselines3.common import atari_wrappers\n",
        "\n",
        "\n",
        "class ImageToPyTorch(gym.ObservationWrapper):\n",
        "    \"\"\"\n",
        "    ImageToPyTorch: Reorders image dimensions from (H, W, C) to (C, H, W)\n",
        "    for compatibility with PyTorch convolutional layers.\n",
        "    \"\"\"\n",
        "    def __init__(self, env):\n",
        "        super(ImageToPyTorch, self).__init__(env)\n",
        "        obs = self.observation_space\n",
        "        assert isinstance(obs, gym.spaces.Box)\n",
        "        assert len(obs.shape) == 3\n",
        "        new_shape = (obs.shape[-1], obs.shape[0], obs.shape[1])\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=obs.low.min(), high=obs.high.max(),\n",
        "            shape=new_shape, dtype=obs.dtype)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        return np.moveaxis(observation, 2, 0)\n",
        "\n",
        "\n",
        "class BufferWrapper(gym.ObservationWrapper):\n",
        "    \"\"\"\n",
        "    BufferWrapper: Maintains a rolling window of the last `n_steps` frames\n",
        "    to give the agent a sense of temporal context.\n",
        "    \"\"\"\n",
        "    def __init__(self, env, n_steps):\n",
        "        super(BufferWrapper, self).__init__(env)\n",
        "        obs = env.observation_space\n",
        "        assert isinstance(obs, spaces.Box)\n",
        "        new_obs = gym.spaces.Box(\n",
        "            obs.low.repeat(n_steps, axis=0), obs.high.repeat(n_steps, axis=0),\n",
        "            dtype=obs.dtype)\n",
        "        self.observation_space = new_obs\n",
        "        self.buffer = collections.deque(maxlen=n_steps)\n",
        "\n",
        "    def reset(self, *, seed: tt.Optional[int] = None, options: tt.Optional[dict[str, tt.Any]] = None):\n",
        "        for _ in range(self.buffer.maxlen):\n",
        "            self.buffer.append(np.zeros_like(self.env.observation_space.low))\n",
        "        obs, extra = self.env.reset()\n",
        "        return self.observation(obs), extra\n",
        "\n",
        "    def observation(self, observation: np.ndarray) -> np.ndarray:\n",
        "        self.buffer.append(observation)\n",
        "        return np.concatenate(self.buffer)\n",
        "\n",
        "\n",
        "def make_env(env_name: str, n_steps=4, render_mode=None, **kwargs):\n",
        "    print(f\"Creating environment {env_name}\")\n",
        "    env = gym.make(env_name, render_mode=render_mode, **kwargs)\n",
        "    env = atari_wrappers.AtariWrapper(env, clip_reward=False, noop_max=0)\n",
        "    env = ImageToPyTorch(env)\n",
        "    env = BufferWrapper(env, n_steps=n_steps)\n",
        "    return env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Only rerun if it hasn't been run before; Prevents resetting the experiment log\n",
        "\n",
        "has_run = 'config_history' in globals() and 'version_counter' in globals()\n",
        "\n",
        "# Experiment logging utility\n",
        "from IPython.display import Markdown, clear_output, display\n",
        "import pandas as pd\n",
        "import datetime\n",
        "\n",
        "# Persistent experiment history\n",
        "if not has_run:\n",
        "    config_history = []\n",
        "    version_counter = 0\n",
        "\n",
        "# Log experiment configurations\n",
        "def log_experiment(config_dict, note=\"\"):\n",
        "    \"\"\"Log experiment configuration and display the table.\"\"\"\n",
        "    global version_counter, config_history\n",
        "\n",
        "    \"\"\"Append config to table and display nicely. Only if it's new.\"\"\"\n",
        "    if(config_history and all(config_dict.get(k) == config_history[-1].get(k) for k in config_dict)):\n",
        "        return  # Skip logging if the config is the same as the last one\n",
        "\n",
        "    entry = {\n",
        "        \"version\": version_counter,\n",
        "        \"timestamp\": datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "        **config_dict,\n",
        "        \"note\": note\n",
        "    }\n",
        "    # Append the new entry to the history\n",
        "    config_history.append(entry)\n",
        "    version_counter += 1\n",
        "\n",
        "    # Display the updated experiment log\n",
        "    clear_output(wait=True)\n",
        "    df = pd.DataFrame(config_history)\n",
        "    display(Markdown(df.to_markdown(index=False)))\n",
        "\n",
        "\n",
        "performance_history = []\n",
        "\n",
        "def log_model_performance(frame_index, model_name, performance_metric_dict):\n",
        "    global performance_history, performance_display\n",
        "\n",
        "    entry = {\n",
        "        \"Frame Index\": frame_index,\n",
        "        \"model_name\": model_name,\n",
        "        **performance_metric_dict,\n",
        "        \"timestamp\": datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    }\n",
        "\n",
        "    performance_history.append(entry)\n",
        "\n",
        "    # Update table in-place WITHOUT clearing the whole cell\n",
        "    df = pd.DataFrame(performance_history)\n",
        "    markdown_table = df.to_markdown(index=False)\n",
        "    performance_display.update(Markdown(markdown_table))\n",
        "\n",
        "def remove_last_entry():\n",
        "    \"\"\"Remove the last entry from the experiment log.\"\"\"\n",
        "    global config_history, version_counter\n",
        "    if config_history:\n",
        "        config_history.pop()\n",
        "        version_counter -= 1\n",
        "        clear_output(wait=True)\n",
        "        df = pd.DataFrame(config_history)\n",
        "        display(Markdown(df.to_markdown(index=False)))\n",
        "\n",
        "def clear_table():\n",
        "    \"\"\"Clear the experiment log table.\"\"\"\n",
        "    global config_history, version_counter\n",
        "    config_history = []\n",
        "    version_counter = 0\n",
        "    clear_output(wait=True)\n",
        "\n",
        "def clear_performance_table():\n",
        "    \"\"\"Clear the performance log table.\"\"\"\n",
        "    global performance_history\n",
        "    performance_history = []\n",
        "    clear_output(wait=True)\n",
        "\n",
        "def print_table_for_github():\n",
        "    \"\"\"Print the experiment log table in markdown format for GitHub.\"\"\"\n",
        "    df = pd.DataFrame(config_history)\n",
        "    print(df.to_markdown(index=False))\n",
        "\n",
        "def snapshot_performance_table(run_label: str = \"\"):\n",
        "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    label = f\"_{run_label}\" if run_label else \"\"\n",
        "    os.makedirs(\"snapshots\", exist_ok=True)\n",
        "\n",
        "    perf_df = pd.DataFrame(performance_history)\n",
        "    perf_path = f\"snapshots/performance_history{label}_{timestamp}.md\"\n",
        "    perf_df.to_markdown(perf_path, index=False)\n",
        "\n",
        "    print(f\"Performance history snapshot saved to {perf_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "|   version | timestamp           | DEFAULT_ENV_NAME   |   MEAN_REWARD_BOUND |   GAMMA |   BATCH_SIZE |   REPLAY_SIZE |   LEARNING_RATE |   SYNC_TARGET_FRAMES |   REPLAY_START_SIZE |   SAVE_EPSILON |   EPSILON_DECAY_LAST_FRAME |   EPSILON_START |   EPSILON_FINAL | note                                                          |\n",
              "|----------:|:--------------------|:-------------------|--------------------:|--------:|-------------:|--------------:|----------------:|---------------------:|--------------------:|---------------:|---------------------------:|----------------:|----------------:|:--------------------------------------------------------------|\n",
              "|         0 | 2025-11-24 07:20:24 | ALE/Pong-v5        |                  19 |    0.99 |           32 |         10000 |          0.0001 |                 1000 |               10000 |            0.5 |                     150000 |               1 |            0.01 | Baseline run                                                  |\n",
              "|         1 | 2025-11-24 08:46:48 | ALE/Pong-v5        |                  19 |    0.99 |           64 |         10000 |          0.0001 |                 1000 |               10000 |            0.5 |                     150000 |               1 |            0.01 | Increase stability, try to help GPU                           |\n",
              "|         2 | 2025-11-24 10:56:18 | ALE/Pong-v5        |                  19 |    0.99 |           32 |         10000 |          0.0002 |                 1000 |               10000 |            0.5 |                     150000 |               1 |            0.01 | Increased Learning rate to encourage more aggressive learning |\n",
              "|         3 | 2025-11-24 15:07:35 | ALE/Pong-v5        |                  19 |    0.99 |           32 |         10000 |          7e-05  |                 1000 |               10000 |            0.5 |                     150000 |               1 |            0.01 | Decreased Learning rate to encourage more stabilized learning |\n",
              "|         4 | 2025-11-24 21:33:35 | ALE/Pong-v5        |                  19 |    0.97 |           32 |         10000 |          0.0001 |                 1000 |               10000 |            0.5 |                     150000 |               1 |            0.01 | Decreased gamma to prioritize more immediate rewards          |"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Removes the last entry entered; only if I mean to\n",
        "can_remove = True\n",
        "\n",
        "if can_remove:\n",
        "    remove_last_entry()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clears the whole table, only if I mean to\n",
        "can_clear = False\n",
        "\n",
        "if can_clear:\n",
        "    clear_table()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clears the whole table, only if I mean to\n",
        "can_clear = False\n",
        "\n",
        "if can_clear:\n",
        "    clear_performance_table()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Prints the table in markdown format\n",
        "print_table_for_github()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "id": "GvXPdjPCBxOd"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "|   version | timestamp           | DEFAULT_ENV_NAME    |   MEAN_REWARD_BOUND |   GAMMA |   BATCH_SIZE |   REPLAY_SIZE |   LEARNING_RATE |   SYNC_TARGET_FRAMES |   REPLAY_START_SIZE |   SAVE_EPSILON |   EPSILON_DECAY_LAST_FRAME |   EPSILON_START |   EPSILON_FINAL | note                                                                            |   Sync_TARGET_FRAMES |\n",
              "|----------:|:--------------------|:--------------------|--------------------:|--------:|-------------:|--------------:|----------------:|---------------------:|--------------------:|---------------:|---------------------------:|----------------:|----------------:|:--------------------------------------------------------------------------------|---------------------:|\n",
              "|         0 | 2025-11-24 07:20:24 | ALE/Pong-v5         |                  19 |    0.99 |           32 |         10000 |          0.0001 |                 1000 |               10000 |            0.5 |                     150000 |               1 |           0.01  | Baseline run                                                                    |                  nan |\n",
              "|         1 | 2025-11-24 08:46:48 | ALE/Pong-v5         |                  19 |    0.99 |           64 |         10000 |          0.0001 |                 1000 |               10000 |            0.5 |                     150000 |               1 |           0.01  | Increase stability, try to help GPU                                             |                  nan |\n",
              "|         2 | 2025-11-24 10:56:18 | ALE/Pong-v5         |                  19 |    0.99 |           32 |         10000 |          0.0002 |                 1000 |               10000 |            0.5 |                     150000 |               1 |           0.01  | Increased Learning rate to encourage more aggressive learning                   |                  nan |\n",
              "|         3 | 2025-11-24 15:07:35 | ALE/Pong-v5         |                  19 |    0.99 |           32 |         10000 |          7e-05  |                 1000 |               10000 |            0.5 |                     150000 |               1 |           0.01  | Decreased Learning rate to encourage more stabilized learning                   |                  nan |\n",
              "|         4 | 2025-11-24 21:33:35 | ALE/Pong-v5         |                  19 |    0.97 |           32 |         10000 |          0.0001 |                 1000 |               10000 |            0.5 |                     150000 |               1 |           0.01  | Decreased gamma to prioritize more immediate rewards                            |                  nan |\n",
              "|         5 | 2025-11-24 22:59:08 | ALE/Pong-v5         |                  19 |    0.99 |           32 |         10000 |          0.0001 |                  500 |               10000 |            0.5 |                     150000 |               1 |           0.01  | Decreased sync target frames to encourage more frequent updates                 |                  500 |\n",
              "|         6 | 2025-11-25 05:17:38 | ALE/Pong-v5         |                  19 |    0.99 |           32 |         10000 |          0.0001 |                 1500 |               10000 |            0.5 |                     150000 |               1 |           0.01  | Increased sync target frames to encourage less frequent and more stable updates |                  500 |\n",
              "|         7 | 2025-11-25 09:21:56 | ALE/Pong-v5         |                  19 |    0.99 |           32 |         10000 |          0.0001 |                 1000 |               10000 |            0.5 |                     150000 |               1 |           0.005 | Decrease epsilon final to make agent greedier and less random                   |                  500 |\n",
              "|         8 | 2025-11-25 10:46:36 | ALE/Pong-v5         |                  19 |    0.99 |           32 |         10000 |          0.0001 |                 1000 |               10000 |            0.5 |                     150000 |               1 |           0.02  | Increase epsilon final to make agent less greedy and more random                |                  500 |\n",
              "|         9 | 2025-11-25 13:27:01 | ALE/VideoPinball-v5 |                  19 |    0.99 |           32 |         10000 |          0.0001 |                 1000 |               10000 |            0.5 |                     150000 |               1 |           0.01  | Baseline run                                                                    |                  nan |\n",
              "|        10 | 2025-11-25 13:34:50 | ALE/VideoPinball-v5 |                5000 |    0.99 |           32 |         10000 |          0.0001 |                 1000 |               10000 |            0.5 |                     150000 |               1 |           0.01  | Baseline run                                                                    |                  nan |\n",
              "|        11 | 2025-11-25 13:49:41 | ALE/Breakout-v5     |                  19 |    0.99 |           32 |         10000 |          0.0001 |                 1000 |               10000 |            0.5 |                     150000 |               1 |           0.01  | Baseline run                                                                    |                  nan |"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Base Configuration\n",
        "config = {\n",
        "    \"DEFAULT_ENV_NAME\": \"ALE/Breakout-v5\",\n",
        "    \"MEAN_REWARD_BOUND\": 19,\n",
        "\n",
        "    \"GAMMA\": 0.99,\n",
        "    \"BATCH_SIZE\": 32,\n",
        "    \"REPLAY_SIZE\": 10000,\n",
        "    \"LEARNING_RATE\": 1e-4,\n",
        "    \"SYNC_TARGET_FRAMES\": 1000,\n",
        "    \"REPLAY_START_SIZE\": 10000,\n",
        "\n",
        "    \"SAVE_EPSILON\": 0.5,  # Only save if at least this much better\n",
        "    \"EPSILON_DECAY_LAST_FRAME\": 150000,\n",
        "    \"EPSILON_START\": 1.0,\n",
        "    \"EPSILON_FINAL\": 0.01\n",
        "}\n",
        "\n",
        "# Log automatically when you rerun this cell\n",
        "log_experiment(config, note=\"Baseline run\")\n",
        "\n",
        "# Tuple of tensors returned from a sampled minibatch in replay buffer\n",
        "State = np.ndarray\n",
        "Action = int\n",
        "BatchTensors = tt.Tuple[\n",
        "    torch.ByteTensor,           # current state\n",
        "    torch.LongTensor,           # actions\n",
        "    torch.Tensor,               # rewards\n",
        "    torch.BoolTensor,           # done || trunc\n",
        "    torch.ByteTensor            # next state\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FHvHNMrR9Sk"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "|   version | timestamp           | DEFAULT_ENV_NAME   |   MEAN_REWARD_BOUND |   GAMMA |   BATCH_SIZE |   REPLAY_SIZE |   LEARNING_RATE |   SYNC_TARGET_FRAMES |   REPLAY_START_SIZE |   SAVE_EPSILON |   EPSILON_DECAY_LAST_FRAME |   EPSILON_START |   EPSILON_FINAL | note                                                                            |   Sync_TARGET_FRAMES |\n",
              "|----------:|:--------------------|:-------------------|--------------------:|--------:|-------------:|--------------:|----------------:|---------------------:|--------------------:|---------------:|---------------------------:|----------------:|----------------:|:--------------------------------------------------------------------------------|---------------------:|\n",
              "|         0 | 2025-11-24 07:20:24 | ALE/Pong-v5        |                  19 |    0.99 |           32 |         10000 |          0.0001 |                 1000 |               10000 |            0.5 |                     150000 |               1 |           0.01  | Baseline run                                                                    |                  nan |\n",
              "|         1 | 2025-11-24 08:46:48 | ALE/Pong-v5        |                  19 |    0.99 |           64 |         10000 |          0.0001 |                 1000 |               10000 |            0.5 |                     150000 |               1 |           0.01  | Increase stability, try to help GPU                                             |                  nan |\n",
              "|         2 | 2025-11-24 10:56:18 | ALE/Pong-v5        |                  19 |    0.99 |           32 |         10000 |          0.0002 |                 1000 |               10000 |            0.5 |                     150000 |               1 |           0.01  | Increased Learning rate to encourage more aggressive learning                   |                  nan |\n",
              "|         3 | 2025-11-24 15:07:35 | ALE/Pong-v5        |                  19 |    0.99 |           32 |         10000 |          7e-05  |                 1000 |               10000 |            0.5 |                     150000 |               1 |           0.01  | Decreased Learning rate to encourage more stabilized learning                   |                  nan |\n",
              "|         4 | 2025-11-24 21:33:35 | ALE/Pong-v5        |                  19 |    0.97 |           32 |         10000 |          0.0001 |                 1000 |               10000 |            0.5 |                     150000 |               1 |           0.01  | Decreased gamma to prioritize more immediate rewards                            |                  nan |\n",
              "|         5 | 2025-11-24 22:59:08 | ALE/Pong-v5        |                  19 |    0.99 |           32 |         10000 |          0.0001 |                  500 |               10000 |            0.5 |                     150000 |               1 |           0.01  | Decreased sync target frames to encourage more frequent updates                 |                  500 |\n",
              "|         6 | 2025-11-25 05:17:38 | ALE/Pong-v5        |                  19 |    0.99 |           32 |         10000 |          0.0001 |                 1500 |               10000 |            0.5 |                     150000 |               1 |           0.01  | Increased sync target frames to encourage less frequent and more stable updates |                  500 |\n",
              "|         7 | 2025-11-25 09:21:56 | ALE/Pong-v5        |                  19 |    0.99 |           32 |         10000 |          0.0001 |                 1000 |               10000 |            0.5 |                     150000 |               1 |           0.005 | Decrease epsilon final to make agent greedier and less random                   |                  500 |\n",
              "|         8 | 2025-11-25 10:46:36 | ALE/Pong-v5        |                  19 |    0.99 |           32 |         10000 |          0.0001 |                 1000 |               10000 |            0.5 |                     150000 |               1 |           0.02  | Increase epsilon final to make agent less greedy and more random                |                  500 |"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# ⚙️ Fast Training Config for Quick Test Run\n",
        "#config[\"MEAN_REWARD_BOUND\"] = 2\n",
        "#config[\"REPLAY_START_SIZE\"] = 1000\n",
        "#config[\"EPSILON_DECAY_LAST_FRAME\"] = 10000\n",
        "#config[\"SYNC_TARGET_FRAMES\"] = 500\n",
        "\n",
        "config[\"BATCH_SIZE\"] = 32               # 1 Increasing to 64 helped single dqn very much, harmed double dqn minorly\n",
        "config[\"LEARNING_RATE\"] = 1e-4          # 2 Increased to 2e-4 to have more aggressive learning, harmed both single and double dqn. Double did not learn\n",
        "config[\"LEARNING_RATE\"] = 1e-4          # 3 Decreased to 7e-5 to have more stablized learning, single-DQN seems to benefit and double-DQN is suffered majorly\n",
        "config[\"GAMMA\"] = 0.99                  # 4 Decreased gamma to 0.97 to prioritize more immediate rewards\n",
        "config[\"SYNC_TARGET_FRAMES\"] = 1000     # 5 Decreased to 500 to sync more frequently, minor benefit for single-DQN massive harm for double-DQN\n",
        "config[\"SYNC_TARGET_FRAMES\"] = 1000     # 6 Increased to 1500 to help stablize values, single-DQN benefited majorly while double-DQN suffer minor reduction\n",
        "config[\"EPSILON_FINAL\"] = 0.01         # 7 Decreased to 0.005 to make agent greedier, less random. single-DQN seemed to benefit while double-DQN suffered major reduction\n",
        "config[\"EPSILON_FINAL\"] = 0.01         # 8 Increased to 0.02 to make agent less greedy, more random.\n",
        "\n",
        "# Log automatically when you rerun this cell\n",
        "log_experiment(config, note=\"Increase epsilon final to make agent less greedy and more random\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {},
      "outputs": [],
      "source": [
        "# When an index value is passed in, make a change to the config\n",
        "\n",
        "def next_change(index):\n",
        "    # Reset all possible changes before next experiment\n",
        "    config[\"BATCH_SIZE\"] = 32\n",
        "    config[\"LEARNING_RATE\"] = 1e-4\n",
        "    config[\"GAMMA\"] = 0.99\n",
        "    config[\"SYNC_TARGET_FRAMES\"] = 1000\n",
        "    config[\"EPSILON_FINAL\"] = 0.01\n",
        "\n",
        "    match index:\n",
        "        case 1:\n",
        "            config[\"BATCH_SIZE\"] = 64               # 1 Increasing to 64 helped single dqn very much, harmed double dqn minorly\n",
        "        case 2:\n",
        "            config[\"LEARNING_RATE\"] = 2e-4          # 2 Increased to 2e-4 to have more aggressive learning, harmed both single and double dqn. Double did not learn\n",
        "        case 3:\n",
        "            config[\"LEARNING_RATE\"] = 7e-5          # 3 Decreased to 7e-5 to have more stablized learning, single-DQN seems to benefit and double-DQN is suffered majorly\n",
        "        case 4:\n",
        "            config[\"GAMMA\"] = 0.97                  # 4 Decreased gamma to 0.97 to prioritize more immediate rewards\n",
        "        case 5:\n",
        "            config[\"SYNC_TARGET_FRAMES\"] = 500     # 5 Decreased to 500 to sync more frequently, minor benefit for single-DQN massive harm for double-DQN\n",
        "        case 6:\n",
        "            config[\"SYNC_TARGET_FRAMES\"] = 1500     # 6 Increased to 1500 to help stablize values, single-DQN benefited majorly while double-DQN suffer minor reduction\n",
        "        case 7:\n",
        "            config[\"EPSILON_FINAL\"] = 0.005         # 7 Decreased to 0.005 to make agent greedier, less random. single-DQN seemed to benefit while double-DQN suffered major reduction\n",
        "        case 8:\n",
        "            config[\"EPSILON_FINAL\"] = 0.02         # 8 Increased to 0.02 to make agent less greedy, more random.\n",
        "        case _:\n",
        "            print(\"Running baseline\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "id": "MkILCuT2OhBV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Define directories\n",
        "save_dir_local = \"saved_models\"\n",
        "\n",
        "# Create both directories if they don't exist\n",
        "os.makedirs(save_dir_local, exist_ok=True)\n",
        "\n",
        "# Safe model filename\n",
        "env_name = config[\"DEFAULT_ENV_NAME\"]\n",
        "safe_env_name = env_name.replace(\"/\", \"_\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "uW4Bo-mcB6rc"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class Experience:\n",
        "    state: State\n",
        "    action: Action\n",
        "    reward: float\n",
        "    done_trunc: bool\n",
        "    new_state: State\n",
        "\n",
        "\n",
        "class ExperienceBuffer:\n",
        "    def __init__(self, capacity: int):\n",
        "        self.buffer = collections.deque(maxlen=capacity)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def append(self, experience: Experience):\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    def sample(self, batch_size: int) -> tt.List[Experience]:\n",
        "        indices = np.random.choice(len(self), batch_size, replace=False)\n",
        "        return [self.buffer[idx] for idx in indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "irJb4V32B-R8"
      },
      "outputs": [],
      "source": [
        "class Agent:\n",
        "    def __init__(self, env: gym.Env, exp_buffer: ExperienceBuffer):\n",
        "        self.env = env\n",
        "        self.exp_buffer = exp_buffer\n",
        "        self.state: tt.Optional[np.ndarray] = None\n",
        "        self._reset()\n",
        "\n",
        "    def _reset(self):\n",
        "        self.state, _ = self.env.reset()\n",
        "        self.total_reward = 0.0\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def play_step(self, net: DQN, device: torch.device, epsilon: float = 0.0) -> tt.Optional[float]:\n",
        "        done_reward = None\n",
        "\n",
        "        # epsilon-greedy action\n",
        "        if np.random.random() < epsilon:\n",
        "            action = self.env.action_space.sample()\n",
        "        else:\n",
        "            state_v = torch.as_tensor(self.state).unsqueeze(0).to(device)  # [1, C, H, W]\n",
        "            q_vals_v = net(state_v)\n",
        "            _, act_v = torch.max(q_vals_v, dim=1)\n",
        "            action = int(act_v.item())\n",
        "\n",
        "        # step env\n",
        "        new_state, reward, done, trunc, _ = self.env.step(action)\n",
        "        self.total_reward += reward\n",
        "\n",
        "        # store raw numpy states in buffer\n",
        "        self.exp_buffer.append(Experience(\n",
        "            state=self.state,\n",
        "            action=action,\n",
        "            reward=float(reward),\n",
        "            done_trunc=done or trunc,\n",
        "            new_state=new_state,\n",
        "        ))\n",
        "\n",
        "        self.state = new_state\n",
        "\n",
        "        if done or trunc:\n",
        "            done_reward = self.total_reward\n",
        "            self._reset()\n",
        "\n",
        "        return done_reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "vHXmNr_wCBJm"
      },
      "outputs": [],
      "source": [
        "def batch_to_tensors(batch: tt.List[Experience], device: torch.device) -> BatchTensors:\n",
        "    states, actions, rewards, dones, new_state = [], [], [], [], []\n",
        "    for e in batch:\n",
        "        states.append(e.state)\n",
        "        actions.append(e.action)\n",
        "        rewards.append(e.reward)\n",
        "        dones.append(e.done_trunc)\n",
        "        new_state.append(e.new_state)\n",
        "    states_t = torch.as_tensor(np.asarray(states))\n",
        "    actions_t = torch.LongTensor(actions)\n",
        "    rewards_t = torch.FloatTensor(rewards)\n",
        "    dones_t = torch.BoolTensor(dones)\n",
        "    new_states_t = torch.as_tensor(np.asarray(new_state))\n",
        "    return states_t.to(device), actions_t.to(device), rewards_t.to(device), \\\n",
        "           dones_t.to(device),  new_states_t.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [],
      "source": [
        "USE_DOUBLE_DQN = False #global setting to toggle double dqn\n",
        "\n",
        "def set_double_dqn(use_double: bool):\n",
        "    global USE_DOUBLE_DQN\n",
        "    USE_DOUBLE_DQN = use_double"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "-dbh0431CEXO"
      },
      "outputs": [],
      "source": [
        "def calc_loss(batch: tt.List[Experience], net: DQN, tgt_net: DQN,\n",
        "              device: torch.device) -> torch.Tensor:\n",
        "    states_t, actions_t, rewards_t, dones_t, new_states_t = batch_to_tensors(batch, device)\n",
        "\n",
        "    state_action_values = net(states_t).gather(\n",
        "        1, actions_t.unsqueeze(-1)\n",
        "    ).squeeze(-1)\n",
        "\n",
        "    # Compute the expected Q values\n",
        "    with torch.no_grad():\n",
        "        if USE_DOUBLE_DQN: # Double DQN\n",
        "            next_actions = net(new_states_t).argmax(dim=1)\n",
        "            next_state_values = tgt_net(new_states_t).gather(1, next_actions.unsqueeze(-1)).squeeze(-1)\n",
        "        else: # DQN\n",
        "            next_state_values = tgt_net(new_states_t).max(1)[0]\n",
        "\n",
        "        next_state_values[dones_t] = 0.0\n",
        "\n",
        "    expected_state_action_values = rewards_t + config[\"GAMMA\"] * next_state_values; \n",
        "\n",
        "    return nn.MSELoss()(state_action_values, expected_state_action_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {},
      "outputs": [],
      "source": [
        "def record_policy_video(env_name, model, device, video_filename=\"policy_rollout.mp4\", max_steps=2000):\n",
        "    \"\"\"Run one greedy episode using the given model and save a video.\"\"\"\n",
        "    # Use your make_env to preserve preprocessing\n",
        "    video_env = make_env(env_name, render_mode=\"rgb_array\")\n",
        "    state, _ = video_env.reset(seed=42)\n",
        "    frames, total_reward = [], 0.0\n",
        "\n",
        "    for step in range(max_steps):\n",
        "        # Capture a rendered RGB frame (already handled by render_mode)\n",
        "        frame = video_env.render()\n",
        "        if isinstance(frame, np.ndarray):\n",
        "            frames.append(frame)\n",
        "\n",
        "        # Greedy action\n",
        "        state_v = torch.as_tensor(state).unsqueeze(0).to(device)\n",
        "        q_vals_v = model(state_v)\n",
        "        _, act_v = torch.max(q_vals_v, dim=1)\n",
        "        action = int(act_v.item())\n",
        "\n",
        "        state, reward, terminated, truncated, _ = video_env.step(action)\n",
        "        total_reward += reward\n",
        "        if terminated or truncated:\n",
        "            frames.append(video_env.render())\n",
        "            break\n",
        "\n",
        "    video_env.close()\n",
        "\n",
        "    # Save video\n",
        "    if frames:\n",
        "        os.makedirs(os.path.dirname(video_filename), exist_ok=True)\n",
        "\n",
        "        # 🔹 Pad frames so height is divisible by 16 (prevents FFMPEG warnings)\n",
        "        padded_frames = []\n",
        "        for f in frames:\n",
        "            h, w = f.shape[:2]\n",
        "            new_h = (h + 15) // 16 * 16  # round up to nearest multiple of 16\n",
        "            if new_h != h:\n",
        "                pad = np.zeros((new_h - h, w, 3), dtype=f.dtype)\n",
        "                f = np.vstack([f, pad])\n",
        "            padded_frames.append(f)\n",
        "\n",
        "        # 🔹 Write padded frames quietly\n",
        "        with imageio.get_writer(video_filename, fps=30) as writer:\n",
        "            for f in padded_frames:\n",
        "                writer.append_data(f)\n",
        "\n",
        "        print(f\"Saved gameplay video: {video_filename} | Total reward: {total_reward:.2f}\")\n",
        "    else:\n",
        "        print(\"No frames captured — render_mode may not be supported.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.cuda.is_available(): True\n",
            "torch.cuda.device_count(): 1\n",
            "torch.cuda.current_device(): 0\n",
            "torch.cuda.get_device_name(0): NVIDIA GeForce RTX 2080\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(\"torch.cuda.is_available():\", torch.cuda.is_available())\n",
        "print(\"torch.cuda.device_count():\", torch.cuda.device_count())\n",
        "print(\"torch.cuda.current_device():\", torch.cuda.current_device() if torch.cuda.is_available() else None)\n",
        "print(\"torch.cuda.get_device_name(0):\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Creating environment ALE/Breakout-v5\n",
            "DQN(\n",
            "  (conv): Sequential(\n",
            "    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
            "    (1): ReLU()\n",
            "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
            "    (3): ReLU()\n",
            "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (5): ReLU()\n",
            "    (6): Flatten(start_dim=1, end_dim=-1)\n",
            "  )\n",
            "  (fc): Sequential(\n",
            "    (0): Linear(in_features=3136, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=4, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "|   Frame Index | model_name   |   Finished Games |   Reward |   Epsilon |   Speed (f/s) |   Elapsed Time (min) | timestamp           |\n",
              "|--------------:|:-------------|-----------------:|---------:|----------:|--------------:|---------------------:|:--------------------|\n",
              "|             5 | DQN          |                1 |     0    | 0.999967  |       139.261 |          0.000581777 | 2025-11-26 02:36:24 |\n",
              "|          6260 | DQN          |              770 |     0.51 | 0.958267  |       337.032 |          0.380234    | 2025-11-26 02:36:46 |\n",
              "|         72428 | DQN          |             7311 |     1.03 | 0.517147  |       130.218 |          9.11635     | 2025-11-26 02:45:31 |\n",
              "|         92170 | DQN          |             8600 |     1.56 | 0.385533  |       126.159 |         11.7798      | 2025-11-26 02:48:10 |\n",
              "|        115415 | DQN          |             9852 |     2.07 | 0.230567  |       125.672 |         14.9161      | 2025-11-26 02:51:19 |\n",
              "|        131404 | DQN          |            10554 |     2.59 | 0.123973  |       117.086 |         17.0836      | 2025-11-26 02:53:29 |\n",
              "|        139510 | DQN          |            10859 |     3.11 | 0.0699333 |       130.166 |         18.1771      | 2025-11-26 02:54:34 |\n",
              "|        156881 | DQN          |            11448 |     3.63 | 0.02      |       124.002 |         20.5306      | 2025-11-26 02:56:56 |\n",
              "|        179951 | DQN          |            12174 |     4.14 | 0.02      |       122.719 |         23.655       | 2025-11-26 03:00:03 |"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5: done 1 games, reward 0.000, eps 1.00, speed 139.26 f/s, time 0.0 min\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Breakout-v5-best_0-20251126-0236-test_epsdec150000_rs10000_sync1000.dat\n",
            "Creating environment ALE/Breakout-v5\n",
            "Saved gameplay video: videos/Breakout/DQN/ALE_Breakout-v5-first-0-20251126-0236.mp4 | Total reward: 0.00\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Breakout-v5-best_0-20251126-0236-test_epsdec150000_rs10000_sync1000.dat\n",
            "Creating environment ALE/Breakout-v5\n",
            "Saved gameplay video: videos/Breakout/DQN/ALE_Breakout-v5-first-0-20251126-0236.mp4 | Total reward: 0.00\n",
            "6260: done 770 games, reward 0.510, eps 0.96, speed 337.03 f/s, time 0.4 min\n",
            "6260: done 770 games, reward 0.510, eps 0.96, speed 337.03 f/s, time 0.4 min\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Breakout-v5-best_0-20251126-0236-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated 0.000 -> 0.510\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Breakout-v5-best_0-20251126-0236-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated 0.000 -> 0.510\n",
            "72428: done 7311 games, reward 1.030, eps 0.52, speed 130.22 f/s, time 9.1 min\n",
            "72428: done 7311 games, reward 1.030, eps 0.52, speed 130.22 f/s, time 9.1 min\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Breakout-v5-best_1-20251126-0245-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated 0.510 -> 1.030\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Breakout-v5-best_1-20251126-0245-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated 0.510 -> 1.030\n",
            "92170: done 8600 games, reward 1.560, eps 0.39, speed 126.16 f/s, time 11.8 min\n",
            "92170: done 8600 games, reward 1.560, eps 0.39, speed 126.16 f/s, time 11.8 min\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Breakout-v5-best_1-20251126-0248-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated 1.030 -> 1.560\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Breakout-v5-best_1-20251126-0248-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated 1.030 -> 1.560\n",
            "115415: done 9852 games, reward 2.070, eps 0.23, speed 125.67 f/s, time 14.9 min\n",
            "115415: done 9852 games, reward 2.070, eps 0.23, speed 125.67 f/s, time 14.9 min\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Breakout-v5-best_2-20251126-0251-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated 1.560 -> 2.070\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Breakout-v5-best_2-20251126-0251-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated 1.560 -> 2.070\n",
            "131404: done 10554 games, reward 2.590, eps 0.12, speed 117.09 f/s, time 17.1 min\n",
            "131404: done 10554 games, reward 2.590, eps 0.12, speed 117.09 f/s, time 17.1 min\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Breakout-v5-best_2-20251126-0253-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated 2.070 -> 2.590\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Breakout-v5-best_2-20251126-0253-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated 2.070 -> 2.590\n",
            "139510: done 10859 games, reward 3.110, eps 0.07, speed 130.17 f/s, time 18.2 min\n",
            "139510: done 10859 games, reward 3.110, eps 0.07, speed 130.17 f/s, time 18.2 min\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Breakout-v5-best_3-20251126-0254-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated 2.590 -> 3.110\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Breakout-v5-best_3-20251126-0254-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated 2.590 -> 3.110\n",
            "156881: done 11448 games, reward 3.630, eps 0.02, speed 124.00 f/s, time 20.5 min\n",
            "156881: done 11448 games, reward 3.630, eps 0.02, speed 124.00 f/s, time 20.5 min\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Breakout-v5-best_3-20251126-0256-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated 3.110 -> 3.630\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Breakout-v5-best_3-20251126-0256-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated 3.110 -> 3.630\n",
            "179951: done 12174 games, reward 4.140, eps 0.02, speed 122.72 f/s, time 23.7 min\n",
            "179951: done 12174 games, reward 4.140, eps 0.02, speed 122.72 f/s, time 23.7 min\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Breakout-v5-best_4-20251126-0300-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated 3.630 -> 4.140\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Breakout-v5-best_4-20251126-0300-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated 3.630 -> 4.140\n",
            "Stopping early at 300000 frames (MAX_FRAMES reached).\n",
            "Creating environment ALE/Breakout-v5\n",
            "Stopping early at 300000 frames (MAX_FRAMES reached).\n",
            "Creating environment ALE/Breakout-v5\n",
            "Saved gameplay video: videos/Breakout/DQN/ALE_Breakout-v5-final-20251126-0316.mp4 | Total reward: 7.00\n",
            "Using device: cuda\n",
            "Creating environment ALE/Breakout-v5\n",
            "Saved gameplay video: videos/Breakout/DQN/ALE_Breakout-v5-final-20251126-0316.mp4 | Total reward: 7.00\n",
            "Using device: cuda\n",
            "Creating environment ALE/Breakout-v5\n",
            "DQN(\n",
            "  (conv): Sequential(\n",
            "    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
            "    (1): ReLU()\n",
            "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
            "    (3): ReLU()\n",
            "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (5): ReLU()\n",
            "    (6): Flatten(start_dim=1, end_dim=-1)\n",
            "  )\n",
            "  (fc): Sequential(\n",
            "    (0): Linear(in_features=3136, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=4, bias=True)\n",
            "  )\n",
            ")\n",
            "DQN(\n",
            "  (conv): Sequential(\n",
            "    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
            "    (1): ReLU()\n",
            "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
            "    (3): ReLU()\n",
            "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (5): ReLU()\n",
            "    (6): Flatten(start_dim=1, end_dim=-1)\n",
            "  )\n",
            "  (fc): Sequential(\n",
            "    (0): Linear(in_features=3136, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=4, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "|   Frame Index | model_name   |   Finished Games |   Reward |   Epsilon |   Speed (f/s) |   Elapsed Time (min) | timestamp           |\n",
              "|--------------:|:-------------|-----------------:|---------:|----------:|--------------:|---------------------:|:--------------------|\n",
              "|             5 | DQN          |                1 | 0        | 0.999967  |       139.261 |          0.000581777 | 2025-11-26 02:36:24 |\n",
              "|          6260 | DQN          |              770 | 0.51     | 0.958267  |       337.032 |          0.380234    | 2025-11-26 02:36:46 |\n",
              "|         72428 | DQN          |             7311 | 1.03     | 0.517147  |       130.218 |          9.11635     | 2025-11-26 02:45:31 |\n",
              "|         92170 | DQN          |             8600 | 1.56     | 0.385533  |       126.159 |         11.7798      | 2025-11-26 02:48:10 |\n",
              "|        115415 | DQN          |             9852 | 2.07     | 0.230567  |       125.672 |         14.9161      | 2025-11-26 02:51:19 |\n",
              "|        131404 | DQN          |            10554 | 2.59     | 0.123973  |       117.086 |         17.0836      | 2025-11-26 02:53:29 |\n",
              "|        139510 | DQN          |            10859 | 3.11     | 0.0699333 |       130.166 |         18.1771      | 2025-11-26 02:54:34 |\n",
              "|        156881 | DQN          |            11448 | 3.63     | 0.02      |       124.002 |         20.5306      | 2025-11-26 02:56:56 |\n",
              "|        179951 | DQN          |            12174 | 4.14     | 0.02      |       122.719 |         23.655       | 2025-11-26 03:00:03 |\n",
              "|             5 | Double DQN   |                1 | 0        | 0.999967  |       208.898 |          0.000365686 | 2025-11-26 03:16:26 |\n",
              "|           120 | Double DQN   |               11 | 0.636364 | 0.9992    |       340.619 |          0.00987518  | 2025-11-26 03:16:27 |\n",
              "|         73507 | Double DQN   |             7264 | 1.14     | 0.509953  |       117.962 |          9.94019     | 2025-11-26 03:26:22 |\n",
              "|         92757 | Double DQN   |             8506 | 1.65     | 0.38162   |       121.384 |         12.6894      | 2025-11-26 03:29:07 |\n",
              "|        113884 | Double DQN   |             9676 | 2.21     | 0.240773  |       119.649 |         15.727       | 2025-11-26 03:32:10 |\n",
              "|        129095 | Double DQN   |            10375 | 2.77     | 0.139367  |       116.44  |         17.9252      | 2025-11-26 03:34:21 |\n",
              "|        143236 | Double DQN   |            10902 | 3.29     | 0.0450933 |       104.193 |         19.9682      | 2025-11-26 03:36:24 |\n",
              "|        148268 | Double DQN   |            11056 | 3.82     | 0.02      |       117.097 |         20.6939      | 2025-11-26 03:37:08 |\n",
              "|        185667 | Double DQN   |            12214 | 4.34     | 0.02      |       113.927 |         26.0984      | 2025-11-26 03:42:32 |\n",
              "|        230309 | Double DQN   |            13606 | 4.88     | 0.02      |       112.292 |         32.5514      | 2025-11-26 03:48:59 |"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5: done 1 games, reward 0.000, eps 1.00, speed 208.90 f/s, time 0.0 min\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Breakout-v5-best_0-20251126-0316-test_epsdec150000_rs10000_sync1000.dat\n",
            "Creating environment ALE/Breakout-v5\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Breakout-v5-best_0-20251126-0316-test_epsdec150000_rs10000_sync1000.dat\n",
            "Creating environment ALE/Breakout-v5\n",
            "Saved gameplay video: videos/Breakout/Double DQN/ALE_Breakout-v5-first-0-20251126-0316.mp4 | Total reward: 0.00\n",
            "Saved gameplay video: videos/Breakout/Double DQN/ALE_Breakout-v5-first-0-20251126-0316.mp4 | Total reward: 0.00\n",
            "120: done 11 games, reward 0.636, eps 1.00, speed 340.62 f/s, time 0.0 min\n",
            "120: done 11 games, reward 0.636, eps 1.00, speed 340.62 f/s, time 0.0 min\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Breakout-v5-best_0-20251126-0316-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated 0.000 -> 0.636\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Breakout-v5-best_0-20251126-0316-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated 0.000 -> 0.636\n",
            "73507: done 7264 games, reward 1.140, eps 0.51, speed 117.96 f/s, time 9.9 min\n",
            "73507: done 7264 games, reward 1.140, eps 0.51, speed 117.96 f/s, time 9.9 min\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Breakout-v5-best_1-20251126-0326-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated 0.636 -> 1.140\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Breakout-v5-best_1-20251126-0326-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated 0.636 -> 1.140\n",
            "92757: done 8506 games, reward 1.650, eps 0.38, speed 121.38 f/s, time 12.7 min\n",
            "92757: done 8506 games, reward 1.650, eps 0.38, speed 121.38 f/s, time 12.7 min\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Breakout-v5-best_1-20251126-0329-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated 1.140 -> 1.650\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Breakout-v5-best_1-20251126-0329-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated 1.140 -> 1.650\n",
            "113884: done 9676 games, reward 2.210, eps 0.24, speed 119.65 f/s, time 15.7 min\n",
            "113884: done 9676 games, reward 2.210, eps 0.24, speed 119.65 f/s, time 15.7 min\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Breakout-v5-best_2-20251126-0332-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated 1.650 -> 2.210\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Breakout-v5-best_2-20251126-0332-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated 1.650 -> 2.210\n",
            "129095: done 10375 games, reward 2.770, eps 0.14, speed 116.44 f/s, time 17.9 min\n",
            "129095: done 10375 games, reward 2.770, eps 0.14, speed 116.44 f/s, time 17.9 min\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Breakout-v5-best_2-20251126-0334-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated 2.210 -> 2.770\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Breakout-v5-best_2-20251126-0334-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated 2.210 -> 2.770\n",
            "143236: done 10902 games, reward 3.290, eps 0.05, speed 104.19 f/s, time 20.0 min\n",
            "143236: done 10902 games, reward 3.290, eps 0.05, speed 104.19 f/s, time 20.0 min\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Breakout-v5-best_3-20251126-0336-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated 2.770 -> 3.290\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Breakout-v5-best_3-20251126-0336-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated 2.770 -> 3.290\n",
            "148268: done 11056 games, reward 3.820, eps 0.02, speed 117.10 f/s, time 20.7 min\n",
            "148268: done 11056 games, reward 3.820, eps 0.02, speed 117.10 f/s, time 20.7 min\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Breakout-v5-best_3-20251126-0337-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated 3.290 -> 3.820\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Breakout-v5-best_3-20251126-0337-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated 3.290 -> 3.820\n",
            "185667: done 12214 games, reward 4.340, eps 0.02, speed 113.93 f/s, time 26.1 min\n",
            "185667: done 12214 games, reward 4.340, eps 0.02, speed 113.93 f/s, time 26.1 min\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Breakout-v5-best_4-20251126-0342-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated 3.820 -> 4.340\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Breakout-v5-best_4-20251126-0342-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated 3.820 -> 4.340\n",
            "230309: done 13606 games, reward 4.880, eps 0.02, speed 112.29 f/s, time 32.6 min\n",
            "230309: done 13606 games, reward 4.880, eps 0.02, speed 112.29 f/s, time 32.6 min\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Breakout-v5-best_4-20251126-0348-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated 4.340 -> 4.880\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Breakout-v5-best_4-20251126-0348-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated 4.340 -> 4.880\n",
            "Stopping early at 300000 frames (MAX_FRAMES reached).\n",
            "Creating environment ALE/Breakout-v5\n",
            "Stopping early at 300000 frames (MAX_FRAMES reached).\n",
            "Creating environment ALE/Breakout-v5\n",
            "Saved gameplay video: videos/Breakout/Double DQN/ALE_Breakout-v5-final-20251126-0359.mp4 | Total reward: 14.00\n",
            "Performance history snapshot saved to snapshots/performance_history_Double DQN_performance_8_20251126-035905.md\n",
            "Saved gameplay video: videos/Breakout/Double DQN/ALE_Breakout-v5-final-20251126-0359.mp4 | Total reward: 14.00\n",
            "Performance history snapshot saved to snapshots/performance_history_Double DQN_performance_8_20251126-035905.md\n"
          ]
        }
      ],
      "source": [
        "# Main training loop with configuration changes\n",
        "for i in range(0,9):\n",
        "    clear_performance_table()\n",
        "    next_change(i)\n",
        "\n",
        "    # Run both DQN and Double DQN for each config change\n",
        "    for j in range(0,2):\n",
        "        if j == 0:\n",
        "            set_double_dqn(False)\n",
        "        else:\n",
        "            set_double_dqn(True)\n",
        "\n",
        "        # Training setup\n",
        "        model_comment = (\n",
        "            f\"test_epsdec{config['EPSILON_DECAY_LAST_FRAME']}\"\n",
        "            f\"_rs{config['REPLAY_START_SIZE']}\"\n",
        "            f\"_sync{config['SYNC_TARGET_FRAMES']}\"\n",
        "        )\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        print(f\"Using device: {device}\")\n",
        "\n",
        "        # Environment and networks\n",
        "        env = make_env(env_name)\n",
        "        net = DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
        "        tgt_net = DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
        "        writer = SummaryWriter(comment=f\"-{env_name}-{model_comment}\")\n",
        "        print(net)\n",
        "\n",
        "        # Experience replay buffer and agent\n",
        "        buffer = ExperienceBuffer(config[\"REPLAY_SIZE\"])\n",
        "        agent = Agent(env, buffer)\n",
        "        epsilon = config[\"EPSILON_START\"]\n",
        "\n",
        "        # Optimizer setup\n",
        "        optimizer = optim.Adam(net.parameters(), lr=config[\"LEARNING_RATE\"])\n",
        "        total_rewards = []\n",
        "        frame_idx = 0\n",
        "        ts_frame = 0\n",
        "        ts = time.time()\n",
        "        best_m_reward = None\n",
        "\n",
        "        if USE_DOUBLE_DQN:\n",
        "            model_name = \"Double DQN\"\n",
        "        else:\n",
        "            model_name = \"DQN\"\n",
        "\n",
        "        performance_metric = {\n",
        "            \"Finished Games\": 0,\n",
        "            \"Reward\": 0.0,\n",
        "            \"Epsilon\": 0.0,\n",
        "            \"Speed (f/s)\": 0.0,\n",
        "            \"Elapsed Time (min)\": 0.0\n",
        "        }\n",
        "\n",
        "        # This creates a placeholder display area that we can update\n",
        "        performance_display = display(Markdown(\"\"), display_id=True)\n",
        "\n",
        "        # Training loop\n",
        "        start_time = time.time()\n",
        "        last_fps_log = start_time\n",
        "        last_fps_frame = frame_idx\n",
        "        FPS_LOG_INTERVAL = 5.0  # seconds\n",
        "        MAX_FRAMES = 300_000  # cap to keep run under ~1–2 hours\n",
        "        while True:\n",
        "            frame_idx += 1\n",
        "            if frame_idx >= MAX_FRAMES:\n",
        "                print(f\"Stopping early at {frame_idx} frames (MAX_FRAMES reached).\")\n",
        "\n",
        "                # Save final video before exit\n",
        "                timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
        "                video_filename = f\"videos/Breakout/{model_name}/{safe_env_name}-final-{timestamp}.mp4\"\n",
        "                os.makedirs(f\"videos/Breakout/{model_name}\", exist_ok=True)\n",
        "                record_policy_video(env_name, net, device, video_filename)\n",
        "                break\n",
        "\n",
        "            # Decay epsilon\n",
        "            epsilon = max(config[\"EPSILON_FINAL\"], config[\"EPSILON_START\"] - frame_idx / config[\"EPSILON_DECAY_LAST_FRAME\"])\n",
        "\n",
        "            reward = agent.play_step(net, device, epsilon)\n",
        "            if reward is not None:\n",
        "                total_rewards.append(reward)\n",
        "                speed = (frame_idx - ts_frame) / (time.time() - ts)\n",
        "                elapsed = time.time() - start_time  # in seconds\n",
        "                ts_frame = frame_idx\n",
        "                ts = time.time()\n",
        "                m_reward = np.mean(total_rewards[-100:])\n",
        "                #  print(f\"{frame_idx}: done {len(total_rewards)} games, reward {m_reward:.3f}, \"\n",
        "                #      f\"eps {epsilon:.2f}, speed {speed:.2f} f/s, time {elapsed/60:.1f} min\")\n",
        "                writer.add_scalar(\"epsilon\", epsilon, frame_idx)\n",
        "                writer.add_scalar(\"speed\", speed, frame_idx)\n",
        "                writer.add_scalar(\"reward_100\", m_reward, frame_idx)\n",
        "                writer.add_scalar(\"reward\", reward, frame_idx)\n",
        "                if best_m_reward is None or m_reward > best_m_reward + config[\"SAVE_EPSILON\"]:\n",
        "                    print(f\"{frame_idx}: done {len(total_rewards)} games, reward {m_reward:.3f}, \"\n",
        "                        f\"eps {epsilon:.2f}, speed {speed:.2f} f/s, time {elapsed/60:.1f} min\")\n",
        "                    \n",
        "                    # Update performance metrics\n",
        "                    performance_metric[\"Finished Games\"] = len(total_rewards)\n",
        "                    performance_metric[\"Reward\"] = m_reward\n",
        "                    performance_metric[\"Epsilon\"] = epsilon\n",
        "                    performance_metric[\"Speed (f/s)\"] = speed\n",
        "                    performance_metric[\"Elapsed Time (min)\"] = elapsed / 60.0\n",
        "\n",
        "                    # Log model performance\n",
        "                    log_model_performance(frame_idx, model_name, performance_metric)\n",
        "\n",
        "                    # Save the model\n",
        "                    timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
        "                    model_filename = f\"{safe_env_name}-best_{int(m_reward)}-{timestamp}-{model_comment}.dat\"\n",
        "                    model_path_local = os.path.join(save_dir_local, model_filename)\n",
        "\n",
        "                    torch.save(net.state_dict(), model_path_local)\n",
        "\n",
        "                    print(f\"Model saved to:\\n - Local:        {model_path_local}\")\n",
        "                    if best_m_reward is not None:\n",
        "                        print(f\"Best reward updated {best_m_reward:.3f} -> {m_reward:.3f}\")\n",
        "                    best_m_reward = m_reward\n",
        "\n",
        "                    # Record first video of training\n",
        "                    if len(total_rewards) == 1:\n",
        "                        video_filename = f\"videos/Breakout/{model_name}/{safe_env_name}-first-{int(m_reward)}-{timestamp}.mp4\"\n",
        "                        os.makedirs(f\"videos/Breakout/{model_name}\", exist_ok=True)\n",
        "                        record_policy_video(env_name, net, device, video_filename)\n",
        "                    \n",
        "                if m_reward > config[\"MEAN_REWARD_BOUND\"]:\n",
        "                    print(\"Solved in %d frames!\" % frame_idx)\n",
        "\n",
        "                    # Final video recording before exit\n",
        "                    timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
        "                    video_filename = f\"videos/Breakout/{model_name}/{safe_env_name}-solved-{int(m_reward)}-{timestamp}.mp4\"\n",
        "                    os.makedirs(f\"videos/Breakout/{model_name}\", exist_ok=True)\n",
        "                    record_policy_video(env_name, net, device, video_filename)\n",
        "                    break\n",
        "            if len(buffer) < config[\"REPLAY_START_SIZE\"]:\n",
        "                continue\n",
        "            if frame_idx % config[\"SYNC_TARGET_FRAMES\"] == 0:\n",
        "                tgt_net.load_state_dict(net.state_dict())\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            batch = buffer.sample(config[\"BATCH_SIZE\"])\n",
        "            loss_t = calc_loss(batch, net, tgt_net, device)\n",
        "            loss_t.backward()\n",
        "            optimizer.step()\n",
        "        env.close()\n",
        "        writer.close()\n",
        "\n",
        "        # Since double dqn the last to be tested run, snapshot performance table\n",
        "        if USE_DOUBLE_DQN:\n",
        "            snapshot_performance_table(run_label=f\"{model_name}_performance_{i}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8gj-5woCXEB",
        "outputId": "afd1308e-a016-4b02-b944-4321357467fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Creating environment ALE/Pong-v5\n",
            "DQN(\n",
            "  (conv): Sequential(\n",
            "    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
            "    (1): ReLU()\n",
            "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
            "    (3): ReLU()\n",
            "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (5): ReLU()\n",
            "    (6): Flatten(start_dim=1, end_dim=-1)\n",
            "  )\n",
            "  (fc): Sequential(\n",
            "    (0): Linear(in_features=3136, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=6, bias=True)\n",
            "  )\n",
            ")\n",
            "DQN(\n",
            "  (conv): Sequential(\n",
            "    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
            "    (1): ReLU()\n",
            "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
            "    (3): ReLU()\n",
            "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (5): ReLU()\n",
            "    (6): Flatten(start_dim=1, end_dim=-1)\n",
            "  )\n",
            "  (fc): Sequential(\n",
            "    (0): Linear(in_features=3136, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=6, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "|   Frame Index | model_name   |   Finished Games |   Reward |   Epsilon |   Speed (f/s) |   Elapsed Time (min) | timestamp           |\n",
              "|--------------:|:-------------|-----------------:|---------:|----------:|--------------:|---------------------:|:--------------------|\n",
              "|           189 | DQN          |                1 |   -21    | 0.99874   |       380.871 |           0.00820402 | 2025-11-25 10:47:03 |\n",
              "|         61833 | DQN          |              265 |   -20.47 | 0.58778   |       137.333 |           6.96032    | 2025-11-25 10:54:00 |\n",
              "|        100114 | DQN          |              392 |   -19.96 | 0.332573  |       129.474 |          11.7678     | 2025-11-25 10:58:49 |\n",
              "|        116585 | DQN          |              434 |   -19.45 | 0.222767  |       129.398 |          13.8876     | 2025-11-25 11:00:56 |\n",
              "|        128836 | DQN          |              459 |   -18.9  | 0.141093  |       128.055 |          15.4816     | 2025-11-25 11:02:32 |\n",
              "|        137404 | DQN          |              473 |   -18.38 | 0.0839733 |       128.203 |          16.601      | 2025-11-25 11:03:39 |\n",
              "|        147576 | DQN          |              487 |   -17.81 | 0.02      |       125.607 |          17.9413     | 2025-11-25 11:04:59 |\n",
              "|        157230 | DQN          |              501 |   -17.29 | 0.02      |       125.57  |          19.2158     | 2025-11-25 11:06:16 |\n",
              "|        169478 | DQN          |              518 |   -16.74 | 0.02      |       124.988 |          20.8374     | 2025-11-25 11:07:53 |\n",
              "|        177483 | DQN          |              529 |   -16.2  | 0.02      |       126.296 |          21.8959     | 2025-11-25 11:08:56 |\n",
              "|        183915 | DQN          |              537 |   -15.69 | 0.02      |       126.217 |          22.744      | 2025-11-25 11:09:47 |\n",
              "|        192616 | DQN          |              548 |   -15.15 | 0.02      |       125.772 |          23.8968     | 2025-11-25 11:10:56 |\n",
              "|        207896 | DQN          |              568 |   -14.63 | 0.02      |       127.781 |          25.9151     | 2025-11-25 11:12:58 |\n",
              "|        226463 | DQN          |              593 |   -14.12 | 0.02      |       127.015 |          28.3706     | 2025-11-25 11:15:25 |\n",
              "|        235464 | DQN          |              604 |   -13.59 | 0.02      |       126.134 |          29.5603     | 2025-11-25 11:16:36 |\n",
              "|        248422 | DQN          |              620 |   -13.07 | 0.02      |       125.095 |          31.2771     | 2025-11-25 11:18:19 |\n",
              "|        279055 | DQN          |              659 |   -12.56 | 0.02      |       126.333 |          35.3259     | 2025-11-25 11:22:22 |\n",
              "|        294072 | DQN          |              677 |   -12.01 | 0.02      |       127.425 |          37.3085     | 2025-11-25 11:24:21 |"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "189: done 1 games, reward -21.000, eps 1.00, speed 380.87 f/s, time 0.0 min\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-21-20251125-1047-test_epsdec150000_rs10000_sync1000.dat\n",
            "Creating environment ALE/Pong-v5\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-21-20251125-1047-test_epsdec150000_rs10000_sync1000.dat\n",
            "Creating environment ALE/Pong-v5\n",
            "Saved gameplay video: videos/DQN/ALE_Pong-v5-first--21-20251125-1047.mp4 | Total reward: -21.00\n",
            "Saved gameplay video: videos/DQN/ALE_Pong-v5-first--21-20251125-1047.mp4 | Total reward: -21.00\n",
            "61833: done 265 games, reward -20.470, eps 0.59, speed 137.33 f/s, time 7.0 min\n",
            "61833: done 265 games, reward -20.470, eps 0.59, speed 137.33 f/s, time 7.0 min\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-20-20251125-1054-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated -21.000 -> -20.470\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-20-20251125-1054-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated -21.000 -> -20.470\n",
            "100114: done 392 games, reward -19.960, eps 0.33, speed 129.47 f/s, time 11.8 min\n",
            "100114: done 392 games, reward -19.960, eps 0.33, speed 129.47 f/s, time 11.8 min\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-19-20251125-1058-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated -20.470 -> -19.960\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-19-20251125-1058-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated -20.470 -> -19.960\n",
            "116585: done 434 games, reward -19.450, eps 0.22, speed 129.40 f/s, time 13.9 min\n",
            "116585: done 434 games, reward -19.450, eps 0.22, speed 129.40 f/s, time 13.9 min\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-19-20251125-1100-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated -19.960 -> -19.450\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-19-20251125-1100-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated -19.960 -> -19.450\n",
            "128836: done 459 games, reward -18.900, eps 0.14, speed 128.05 f/s, time 15.5 min\n",
            "128836: done 459 games, reward -18.900, eps 0.14, speed 128.05 f/s, time 15.5 min\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-18-20251125-1102-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated -19.450 -> -18.900\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-18-20251125-1102-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated -19.450 -> -18.900\n",
            "137404: done 473 games, reward -18.380, eps 0.08, speed 128.20 f/s, time 16.6 min\n",
            "137404: done 473 games, reward -18.380, eps 0.08, speed 128.20 f/s, time 16.6 min\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-18-20251125-1103-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated -18.900 -> -18.380\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-18-20251125-1103-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated -18.900 -> -18.380\n",
            "147576: done 487 games, reward -17.810, eps 0.02, speed 125.61 f/s, time 17.9 min\n",
            "147576: done 487 games, reward -17.810, eps 0.02, speed 125.61 f/s, time 17.9 min\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-17-20251125-1104-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated -18.380 -> -17.810\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-17-20251125-1104-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated -18.380 -> -17.810\n",
            "157230: done 501 games, reward -17.290, eps 0.02, speed 125.57 f/s, time 19.2 min\n",
            "157230: done 501 games, reward -17.290, eps 0.02, speed 125.57 f/s, time 19.2 min\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-17-20251125-1106-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated -17.810 -> -17.290\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-17-20251125-1106-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated -17.810 -> -17.290\n",
            "169478: done 518 games, reward -16.740, eps 0.02, speed 124.99 f/s, time 20.8 min\n",
            "169478: done 518 games, reward -16.740, eps 0.02, speed 124.99 f/s, time 20.8 min\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-16-20251125-1107-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated -17.290 -> -16.740\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-16-20251125-1107-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated -17.290 -> -16.740\n",
            "177483: done 529 games, reward -16.200, eps 0.02, speed 126.30 f/s, time 21.9 min\n",
            "177483: done 529 games, reward -16.200, eps 0.02, speed 126.30 f/s, time 21.9 min\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-16-20251125-1108-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated -16.740 -> -16.200\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-16-20251125-1108-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated -16.740 -> -16.200\n",
            "183915: done 537 games, reward -15.690, eps 0.02, speed 126.22 f/s, time 22.7 min\n",
            "183915: done 537 games, reward -15.690, eps 0.02, speed 126.22 f/s, time 22.7 min\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-15-20251125-1109-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated -16.200 -> -15.690\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-15-20251125-1109-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated -16.200 -> -15.690\n",
            "192616: done 548 games, reward -15.150, eps 0.02, speed 125.77 f/s, time 23.9 min\n",
            "192616: done 548 games, reward -15.150, eps 0.02, speed 125.77 f/s, time 23.9 min\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-15-20251125-1110-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated -15.690 -> -15.150\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-15-20251125-1110-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated -15.690 -> -15.150\n",
            "207896: done 568 games, reward -14.630, eps 0.02, speed 127.78 f/s, time 25.9 min\n",
            "207896: done 568 games, reward -14.630, eps 0.02, speed 127.78 f/s, time 25.9 min\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-14-20251125-1112-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated -15.150 -> -14.630\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-14-20251125-1112-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated -15.150 -> -14.630\n",
            "226463: done 593 games, reward -14.120, eps 0.02, speed 127.01 f/s, time 28.4 min\n",
            "226463: done 593 games, reward -14.120, eps 0.02, speed 127.01 f/s, time 28.4 min\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-14-20251125-1115-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated -14.630 -> -14.120\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-14-20251125-1115-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated -14.630 -> -14.120\n",
            "235464: done 604 games, reward -13.590, eps 0.02, speed 126.13 f/s, time 29.6 min\n",
            "235464: done 604 games, reward -13.590, eps 0.02, speed 126.13 f/s, time 29.6 min\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-13-20251125-1116-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated -14.120 -> -13.590\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-13-20251125-1116-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated -14.120 -> -13.590\n",
            "248422: done 620 games, reward -13.070, eps 0.02, speed 125.10 f/s, time 31.3 min\n",
            "248422: done 620 games, reward -13.070, eps 0.02, speed 125.10 f/s, time 31.3 min\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-13-20251125-1118-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated -13.590 -> -13.070\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-13-20251125-1118-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated -13.590 -> -13.070\n",
            "279055: done 659 games, reward -12.560, eps 0.02, speed 126.33 f/s, time 35.3 min\n",
            "279055: done 659 games, reward -12.560, eps 0.02, speed 126.33 f/s, time 35.3 min\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-12-20251125-1122-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated -13.070 -> -12.560\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-12-20251125-1122-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated -13.070 -> -12.560\n",
            "294072: done 677 games, reward -12.010, eps 0.02, speed 127.43 f/s, time 37.3 min\n",
            "294072: done 677 games, reward -12.010, eps 0.02, speed 127.43 f/s, time 37.3 min\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-12-20251125-1124-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated -12.560 -> -12.010\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-12-20251125-1124-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated -12.560 -> -12.010\n",
            "Stopping early at 300000 frames (MAX_FRAMES reached).\n",
            "Creating environment ALE/Pong-v5\n",
            "Stopping early at 300000 frames (MAX_FRAMES reached).\n",
            "Creating environment ALE/Pong-v5\n",
            "Saved gameplay video: videos/DQN/ALE_Pong-v5-final-20251125-1125.mp4 | Total reward: -11.00\n",
            "Saved gameplay video: videos/DQN/ALE_Pong-v5-final-20251125-1125.mp4 | Total reward: -11.00\n"
          ]
        }
      ],
      "source": [
        "#Clear output for next run\n",
        "clear_performance_table()\n",
        "\n",
        "# Training setup\n",
        "model_comment = (\n",
        "    f\"test_epsdec{config['EPSILON_DECAY_LAST_FRAME']}\"\n",
        "    f\"_rs{config['REPLAY_START_SIZE']}\"\n",
        "    f\"_sync{config['SYNC_TARGET_FRAMES']}\"\n",
        " )\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Environment and networks\n",
        "env = make_env(env_name)\n",
        "net = DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
        "tgt_net = DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
        "writer = SummaryWriter(comment=f\"-{env_name}-{model_comment}\")\n",
        "print(net)\n",
        "\n",
        "# Experience replay buffer and agent\n",
        "buffer = ExperienceBuffer(config[\"REPLAY_SIZE\"])\n",
        "agent = Agent(env, buffer)\n",
        "epsilon = config[\"EPSILON_START\"]\n",
        "\n",
        "# Optimizer setup\n",
        "optimizer = optim.Adam(net.parameters(), lr=config[\"LEARNING_RATE\"])\n",
        "total_rewards = []\n",
        "frame_idx = 0\n",
        "ts_frame = 0\n",
        "ts = time.time()\n",
        "best_m_reward = None\n",
        "\n",
        "set_double_dqn(False)  # Set to True to use Double DQN\n",
        "\n",
        "if USE_DOUBLE_DQN:\n",
        "    model_name = \"Double DQN\"\n",
        "else:\n",
        "    model_name = \"DQN\"\n",
        "\n",
        "performance_metric = {\n",
        "    \"Finished Games\": 0,\n",
        "    \"Reward\": 0.0,\n",
        "    \"Epsilon\": 0.0,\n",
        "    \"Speed (f/s)\": 0.0,\n",
        "    \"Elapsed Time (min)\": 0.0\n",
        "}\n",
        "\n",
        "# This creates a placeholder display area that we can update\n",
        "performance_display = display(Markdown(\"\"), display_id=True)\n",
        "\n",
        "# Training loop\n",
        "start_time = time.time()\n",
        "last_fps_log = start_time\n",
        "last_fps_frame = frame_idx\n",
        "FPS_LOG_INTERVAL = 5.0  # seconds\n",
        "MAX_FRAMES = 300_000  # cap to keep run under ~1–2 hours\n",
        "while True:\n",
        "    frame_idx += 1\n",
        "    if frame_idx >= MAX_FRAMES:\n",
        "        print(f\"Stopping early at {frame_idx} frames (MAX_FRAMES reached).\")\n",
        "\n",
        "        # Save final video before exit\n",
        "        timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
        "        video_filename = f\"videos/{model_name}/{safe_env_name}-final-{timestamp}.mp4\"\n",
        "        os.makedirs(f\"videos/{model_name}\", exist_ok=True)\n",
        "        record_policy_video(env_name, net, device, video_filename)\n",
        "        break\n",
        "\n",
        "    # Decay epsilon\n",
        "    epsilon = max(config[\"EPSILON_FINAL\"], config[\"EPSILON_START\"] - frame_idx / config[\"EPSILON_DECAY_LAST_FRAME\"])\n",
        "\n",
        "    reward = agent.play_step(net, device, epsilon)\n",
        "    if reward is not None:\n",
        "        total_rewards.append(reward)\n",
        "        speed = (frame_idx - ts_frame) / (time.time() - ts)\n",
        "        elapsed = time.time() - start_time  # in seconds\n",
        "        ts_frame = frame_idx\n",
        "        ts = time.time()\n",
        "        m_reward = np.mean(total_rewards[-100:])\n",
        "        #  print(f\"{frame_idx}: done {len(total_rewards)} games, reward {m_reward:.3f}, \"\n",
        "        #      f\"eps {epsilon:.2f}, speed {speed:.2f} f/s, time {elapsed/60:.1f} min\")\n",
        "        writer.add_scalar(\"epsilon\", epsilon, frame_idx)\n",
        "        writer.add_scalar(\"speed\", speed, frame_idx)\n",
        "        writer.add_scalar(\"reward_100\", m_reward, frame_idx)\n",
        "        writer.add_scalar(\"reward\", reward, frame_idx)\n",
        "        if best_m_reward is None or m_reward > best_m_reward + config[\"SAVE_EPSILON\"]:\n",
        "            print(f\"{frame_idx}: done {len(total_rewards)} games, reward {m_reward:.3f}, \"\n",
        "                f\"eps {epsilon:.2f}, speed {speed:.2f} f/s, time {elapsed/60:.1f} min\")\n",
        "            \n",
        "            # Update performance metrics\n",
        "            performance_metric[\"Finished Games\"] = len(total_rewards)\n",
        "            performance_metric[\"Reward\"] = m_reward\n",
        "            performance_metric[\"Epsilon\"] = epsilon\n",
        "            performance_metric[\"Speed (f/s)\"] = speed\n",
        "            performance_metric[\"Elapsed Time (min)\"] = elapsed / 60.0\n",
        "\n",
        "            # Log model performance\n",
        "            log_model_performance(frame_idx, model_name, performance_metric)\n",
        "\n",
        "            # Save the model\n",
        "            timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
        "            model_filename = f\"{safe_env_name}-best_{int(m_reward)}-{timestamp}-{model_comment}.dat\"\n",
        "            model_path_local = os.path.join(save_dir_local, model_filename)\n",
        "\n",
        "            torch.save(net.state_dict(), model_path_local)\n",
        "\n",
        "            print(f\"Model saved to:\\n - Local:        {model_path_local}\")\n",
        "            if best_m_reward is not None:\n",
        "                print(f\"Best reward updated {best_m_reward:.3f} -> {m_reward:.3f}\")\n",
        "            best_m_reward = m_reward\n",
        "\n",
        "            # Record first video of training\n",
        "            if len(total_rewards) == 1:\n",
        "                video_filename = f\"videos/Pinball/{model_name}/{safe_env_name}-first-{int(m_reward)}-{timestamp}.mp4\"\n",
        "                os.makedirs(f\"videos/Pinball/{model_name}\", exist_ok=True)\n",
        "                record_policy_video(env_name, net, device, video_filename)\n",
        "            \n",
        "        if m_reward > config[\"MEAN_REWARD_BOUND\"]:\n",
        "            print(\"Solved in %d frames!\" % frame_idx)\n",
        "\n",
        "            # Final video recording before exit\n",
        "            timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
        "            video_filename = f\"videos/Pinball/{model_name}/{safe_env_name}-solved-{int(m_reward)}-{timestamp}.mp4\"\n",
        "            os.makedirs(f\"videos/Pinball/{model_name}\", exist_ok=True)\n",
        "            record_policy_video(env_name, net, device, video_filename)\n",
        "            break\n",
        "    if len(buffer) < config[\"REPLAY_START_SIZE\"]:\n",
        "        continue\n",
        "    if frame_idx % config[\"SYNC_TARGET_FRAMES\"] == 0:\n",
        "        tgt_net.load_state_dict(net.state_dict())\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    batch = buffer.sample(config[\"BATCH_SIZE\"])\n",
        "    loss_t = calc_loss(batch, net, tgt_net, device)\n",
        "    loss_t.backward()\n",
        "    optimizer.step()\n",
        "env.close()\n",
        "writer.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Creating environment ALE/Pong-v5\n",
            "DQN(\n",
            "  (conv): Sequential(\n",
            "    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
            "    (1): ReLU()\n",
            "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
            "    (3): ReLU()\n",
            "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (5): ReLU()\n",
            "    (6): Flatten(start_dim=1, end_dim=-1)\n",
            "  )\n",
            "  (fc): Sequential(\n",
            "    (0): Linear(in_features=3136, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=6, bias=True)\n",
            "  )\n",
            ")\n",
            "DQN(\n",
            "  (conv): Sequential(\n",
            "    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
            "    (1): ReLU()\n",
            "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
            "    (3): ReLU()\n",
            "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (5): ReLU()\n",
            "    (6): Flatten(start_dim=1, end_dim=-1)\n",
            "  )\n",
            "  (fc): Sequential(\n",
            "    (0): Linear(in_features=3136, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=6, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "|   Frame Index | model_name   |   Finished Games |   Reward |   Epsilon |   Speed (f/s) |   Elapsed Time (min) | timestamp           |\n",
              "|--------------:|:-------------|-----------------:|---------:|----------:|--------------:|---------------------:|:--------------------|\n",
              "|           189 | DQN          |                1 |   -21    | 0.99874   |       380.871 |           0.00820402 | 2025-11-25 10:47:03 |\n",
              "|         61833 | DQN          |              265 |   -20.47 | 0.58778   |       137.333 |           6.96032    | 2025-11-25 10:54:00 |\n",
              "|        100114 | DQN          |              392 |   -19.96 | 0.332573  |       129.474 |          11.7678     | 2025-11-25 10:58:49 |\n",
              "|        116585 | DQN          |              434 |   -19.45 | 0.222767  |       129.398 |          13.8876     | 2025-11-25 11:00:56 |\n",
              "|        128836 | DQN          |              459 |   -18.9  | 0.141093  |       128.055 |          15.4816     | 2025-11-25 11:02:32 |\n",
              "|        137404 | DQN          |              473 |   -18.38 | 0.0839733 |       128.203 |          16.601      | 2025-11-25 11:03:39 |\n",
              "|        147576 | DQN          |              487 |   -17.81 | 0.02      |       125.607 |          17.9413     | 2025-11-25 11:04:59 |\n",
              "|        157230 | DQN          |              501 |   -17.29 | 0.02      |       125.57  |          19.2158     | 2025-11-25 11:06:16 |\n",
              "|        169478 | DQN          |              518 |   -16.74 | 0.02      |       124.988 |          20.8374     | 2025-11-25 11:07:53 |\n",
              "|        177483 | DQN          |              529 |   -16.2  | 0.02      |       126.296 |          21.8959     | 2025-11-25 11:08:56 |\n",
              "|        183915 | DQN          |              537 |   -15.69 | 0.02      |       126.217 |          22.744      | 2025-11-25 11:09:47 |\n",
              "|        192616 | DQN          |              548 |   -15.15 | 0.02      |       125.772 |          23.8968     | 2025-11-25 11:10:56 |\n",
              "|        207896 | DQN          |              568 |   -14.63 | 0.02      |       127.781 |          25.9151     | 2025-11-25 11:12:58 |\n",
              "|        226463 | DQN          |              593 |   -14.12 | 0.02      |       127.015 |          28.3706     | 2025-11-25 11:15:25 |\n",
              "|        235464 | DQN          |              604 |   -13.59 | 0.02      |       126.134 |          29.5603     | 2025-11-25 11:16:36 |\n",
              "|        248422 | DQN          |              620 |   -13.07 | 0.02      |       125.095 |          31.2771     | 2025-11-25 11:18:19 |\n",
              "|        279055 | DQN          |              659 |   -12.56 | 0.02      |       126.333 |          35.3259     | 2025-11-25 11:22:22 |\n",
              "|        294072 | DQN          |              677 |   -12.01 | 0.02      |       127.425 |          37.3085     | 2025-11-25 11:24:21 |\n",
              "|           189 | Double DQN   |                1 |   -21    | 0.99874   |       383.613 |           0.00819348 | 2025-11-25 11:25:12 |\n",
              "|         45187 | Double DQN   |              196 |   -20.49 | 0.698753  |       129.199 |           4.90006    | 2025-11-25 11:30:05 |\n",
              "|         71955 | Double DQN   |              294 |   -19.98 | 0.5203    |       127.158 |           8.38083    | 2025-11-25 11:33:34 |\n",
              "|         98318 | Double DQN   |              372 |   -19.46 | 0.344547  |       124.746 |          11.8896     | 2025-11-25 11:37:04 |\n",
              "|        116711 | Double DQN   |              415 |   -18.92 | 0.221927  |       121.176 |          14.3967     | 2025-11-25 11:39:35 |\n",
              "|        124110 | Double DQN   |              430 |   -18.39 | 0.1726    |       121.068 |          15.4164     | 2025-11-25 11:40:36 |\n",
              "|        135643 | Double DQN   |              452 |   -17.82 | 0.0957133 |       118.802 |          17.0113     | 2025-11-25 11:42:12 |\n",
              "|        142087 | Double DQN   |              463 |   -17.26 | 0.0527533 |       118.825 |          17.9132     | 2025-11-25 11:43:06 |\n",
              "|        149166 | Double DQN   |              473 |   -16.68 | 0.02      |       117.338 |          18.9106     | 2025-11-25 11:44:06 |\n",
              "|        155353 | Double DQN   |              482 |   -16.16 | 0.02      |       118.76  |          19.7842     | 2025-11-25 11:44:58 |\n",
              "|        161526 | Double DQN   |              490 |   -15.57 | 0.02      |       118.03  |          20.654      | 2025-11-25 11:45:50 |\n",
              "|        166609 | Double DQN   |              496 |   -15.03 | 0.02      |       116.042 |          21.3746     | 2025-11-25 11:46:34 |\n",
              "|        172506 | Double DQN   |              503 |   -14.47 | 0.02      |       118.004 |          22.207      | 2025-11-25 11:47:23 |\n",
              "|        180518 | Double DQN   |              514 |   -13.94 | 0.02      |       118.837 |          23.338      | 2025-11-25 11:48:31 |\n",
              "|        188625 | Double DQN   |              524 |   -13.41 | 0.02      |       119.177 |          24.4808     | 2025-11-25 11:49:40 |\n",
              "|        195443 | Double DQN   |              533 |   -12.9  | 0.02      |       118.871 |          25.4461     | 2025-11-25 11:50:38 |\n",
              "|        200075 | Double DQN   |              538 |   -12.29 | 0.02      |       117.97  |          26.1009     | 2025-11-25 11:51:17 |\n",
              "|        206233 | Double DQN   |              546 |   -11.78 | 0.02      |       118.768 |          26.9666     | 2025-11-25 11:52:09 |\n",
              "|        212503 | Double DQN   |              554 |   -11.26 | 0.02      |       118.181 |          27.8531     | 2025-11-25 11:53:02 |\n",
              "|        220764 | Double DQN   |              564 |   -10.66 | 0.02      |       119.442 |          29.0163     | 2025-11-25 11:54:12 |\n",
              "|        229945 | Double DQN   |              575 |   -10.13 | 0.02      |       118.492 |          30.3081     | 2025-11-25 11:55:30 |\n",
              "|        236537 | Double DQN   |              582 |    -9.49 | 0.02      |       119.073 |          31.2349     | 2025-11-25 11:56:25 |\n",
              "|        241864 | Double DQN   |              588 |    -8.97 | 0.02      |       118.946 |          31.9852     | 2025-11-25 11:57:10 |\n",
              "|        256378 | Double DQN   |              605 |    -8.43 | 0.02      |       117.505 |          34.0261     | 2025-11-25 11:59:13 |\n",
              "|        260317 | Double DQN   |              609 |    -7.88 | 0.02      |       119.072 |          34.5801     | 2025-11-25 11:59:46 |\n",
              "|        264911 | Double DQN   |              614 |    -7.35 | 0.02      |       116.479 |          35.2269     | 2025-11-25 12:00:25 |\n",
              "|        278067 | Double DQN   |              630 |    -6.84 | 0.02      |       120.133 |          37.0746     | 2025-11-25 12:02:16 |\n",
              "|        287188 | Double DQN   |              640 |    -6.32 | 0.02      |       119.929 |          38.3543     | 2025-11-25 12:03:32 |"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "189: done 1 games, reward -21.000, eps 1.00, speed 383.61 f/s, time 0.0 min\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-21-20251125-1125-test_epsdec150000_rs10000_sync1000.dat\n",
            "Creating environment ALE/Pong-v5\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-21-20251125-1125-test_epsdec150000_rs10000_sync1000.dat\n",
            "Creating environment ALE/Pong-v5\n",
            "Saved gameplay video: videos/Double DQN/ALE_Pong-v5-first--21-20251125-1125.mp4 | Total reward: -21.00\n",
            "Saved gameplay video: videos/Double DQN/ALE_Pong-v5-first--21-20251125-1125.mp4 | Total reward: -21.00\n",
            "45187: done 196 games, reward -20.490, eps 0.70, speed 129.20 f/s, time 4.9 min\n",
            "45187: done 196 games, reward -20.490, eps 0.70, speed 129.20 f/s, time 4.9 min\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-20-20251125-1130-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated -21.000 -> -20.490\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-20-20251125-1130-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated -21.000 -> -20.490\n",
            "71955: done 294 games, reward -19.980, eps 0.52, speed 127.16 f/s, time 8.4 min\n",
            "71955: done 294 games, reward -19.980, eps 0.52, speed 127.16 f/s, time 8.4 min\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-19-20251125-1133-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated -20.490 -> -19.980\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-19-20251125-1133-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated -20.490 -> -19.980\n",
            "98318: done 372 games, reward -19.460, eps 0.34, speed 124.75 f/s, time 11.9 min\n",
            "98318: done 372 games, reward -19.460, eps 0.34, speed 124.75 f/s, time 11.9 min\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-19-20251125-1137-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated -19.980 -> -19.460\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-19-20251125-1137-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated -19.980 -> -19.460\n",
            "116711: done 415 games, reward -18.920, eps 0.22, speed 121.18 f/s, time 14.4 min\n",
            "116711: done 415 games, reward -18.920, eps 0.22, speed 121.18 f/s, time 14.4 min\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-18-20251125-1139-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated -19.460 -> -18.920\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-18-20251125-1139-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated -19.460 -> -18.920\n",
            "124110: done 430 games, reward -18.390, eps 0.17, speed 121.07 f/s, time 15.4 min\n",
            "124110: done 430 games, reward -18.390, eps 0.17, speed 121.07 f/s, time 15.4 min\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-18-20251125-1140-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated -18.920 -> -18.390\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-18-20251125-1140-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated -18.920 -> -18.390\n",
            "135643: done 452 games, reward -17.820, eps 0.10, speed 118.80 f/s, time 17.0 min\n",
            "135643: done 452 games, reward -17.820, eps 0.10, speed 118.80 f/s, time 17.0 min\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-17-20251125-1142-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated -18.390 -> -17.820\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-17-20251125-1142-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated -18.390 -> -17.820\n",
            "142087: done 463 games, reward -17.260, eps 0.05, speed 118.82 f/s, time 17.9 min\n",
            "142087: done 463 games, reward -17.260, eps 0.05, speed 118.82 f/s, time 17.9 min\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-17-20251125-1143-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated -17.820 -> -17.260\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-17-20251125-1143-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated -17.820 -> -17.260\n",
            "149166: done 473 games, reward -16.680, eps 0.02, speed 117.34 f/s, time 18.9 min\n",
            "149166: done 473 games, reward -16.680, eps 0.02, speed 117.34 f/s, time 18.9 min\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-16-20251125-1144-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated -17.260 -> -16.680\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-16-20251125-1144-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated -17.260 -> -16.680\n",
            "155353: done 482 games, reward -16.160, eps 0.02, speed 118.76 f/s, time 19.8 min\n",
            "155353: done 482 games, reward -16.160, eps 0.02, speed 118.76 f/s, time 19.8 min\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-16-20251125-1144-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated -16.680 -> -16.160\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-16-20251125-1144-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated -16.680 -> -16.160\n",
            "161526: done 490 games, reward -15.570, eps 0.02, speed 118.03 f/s, time 20.7 min\n",
            "161526: done 490 games, reward -15.570, eps 0.02, speed 118.03 f/s, time 20.7 min\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-15-20251125-1145-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated -16.160 -> -15.570\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-15-20251125-1145-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated -16.160 -> -15.570\n",
            "166609: done 496 games, reward -15.030, eps 0.02, speed 116.04 f/s, time 21.4 min\n",
            "166609: done 496 games, reward -15.030, eps 0.02, speed 116.04 f/s, time 21.4 min\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-15-20251125-1146-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated -15.570 -> -15.030\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-15-20251125-1146-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated -15.570 -> -15.030\n",
            "172506: done 503 games, reward -14.470, eps 0.02, speed 118.00 f/s, time 22.2 min\n",
            "172506: done 503 games, reward -14.470, eps 0.02, speed 118.00 f/s, time 22.2 min\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-14-20251125-1147-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated -15.030 -> -14.470\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-14-20251125-1147-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated -15.030 -> -14.470\n",
            "180518: done 514 games, reward -13.940, eps 0.02, speed 118.84 f/s, time 23.3 min\n",
            "180518: done 514 games, reward -13.940, eps 0.02, speed 118.84 f/s, time 23.3 min\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-13-20251125-1148-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated -14.470 -> -13.940\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-13-20251125-1148-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated -14.470 -> -13.940\n",
            "188625: done 524 games, reward -13.410, eps 0.02, speed 119.18 f/s, time 24.5 min\n",
            "188625: done 524 games, reward -13.410, eps 0.02, speed 119.18 f/s, time 24.5 min\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-13-20251125-1149-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated -13.940 -> -13.410\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-13-20251125-1149-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated -13.940 -> -13.410\n",
            "195443: done 533 games, reward -12.900, eps 0.02, speed 118.87 f/s, time 25.4 min\n",
            "195443: done 533 games, reward -12.900, eps 0.02, speed 118.87 f/s, time 25.4 min\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-12-20251125-1150-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated -13.410 -> -12.900\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-12-20251125-1150-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated -13.410 -> -12.900\n",
            "200075: done 538 games, reward -12.290, eps 0.02, speed 117.97 f/s, time 26.1 min\n",
            "200075: done 538 games, reward -12.290, eps 0.02, speed 117.97 f/s, time 26.1 min\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-12-20251125-1151-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated -12.900 -> -12.290\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-12-20251125-1151-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated -12.900 -> -12.290\n",
            "206233: done 546 games, reward -11.780, eps 0.02, speed 118.77 f/s, time 27.0 min\n",
            "206233: done 546 games, reward -11.780, eps 0.02, speed 118.77 f/s, time 27.0 min\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-11-20251125-1152-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated -12.290 -> -11.780\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-11-20251125-1152-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated -12.290 -> -11.780\n",
            "212503: done 554 games, reward -11.260, eps 0.02, speed 118.18 f/s, time 27.9 min\n",
            "212503: done 554 games, reward -11.260, eps 0.02, speed 118.18 f/s, time 27.9 min\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-11-20251125-1153-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated -11.780 -> -11.260\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-11-20251125-1153-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated -11.780 -> -11.260\n",
            "220764: done 564 games, reward -10.660, eps 0.02, speed 119.44 f/s, time 29.0 min\n",
            "220764: done 564 games, reward -10.660, eps 0.02, speed 119.44 f/s, time 29.0 min\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-10-20251125-1154-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated -11.260 -> -10.660\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-10-20251125-1154-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated -11.260 -> -10.660\n",
            "229945: done 575 games, reward -10.130, eps 0.02, speed 118.49 f/s, time 30.3 min\n",
            "229945: done 575 games, reward -10.130, eps 0.02, speed 118.49 f/s, time 30.3 min\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-10-20251125-1155-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated -10.660 -> -10.130\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-10-20251125-1155-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated -10.660 -> -10.130\n",
            "236537: done 582 games, reward -9.490, eps 0.02, speed 119.07 f/s, time 31.2 min\n",
            "236537: done 582 games, reward -9.490, eps 0.02, speed 119.07 f/s, time 31.2 min\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-9-20251125-1156-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated -10.130 -> -9.490\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-9-20251125-1156-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated -10.130 -> -9.490\n",
            "241864: done 588 games, reward -8.970, eps 0.02, speed 118.95 f/s, time 32.0 min\n",
            "241864: done 588 games, reward -8.970, eps 0.02, speed 118.95 f/s, time 32.0 min\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-8-20251125-1157-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated -9.490 -> -8.970\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-8-20251125-1157-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated -9.490 -> -8.970\n",
            "256378: done 605 games, reward -8.430, eps 0.02, speed 117.51 f/s, time 34.0 min\n",
            "256378: done 605 games, reward -8.430, eps 0.02, speed 117.51 f/s, time 34.0 min\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-8-20251125-1159-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated -8.970 -> -8.430\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-8-20251125-1159-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated -8.970 -> -8.430\n",
            "260317: done 609 games, reward -7.880, eps 0.02, speed 119.07 f/s, time 34.6 min\n",
            "260317: done 609 games, reward -7.880, eps 0.02, speed 119.07 f/s, time 34.6 min\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-7-20251125-1159-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated -8.430 -> -7.880\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-7-20251125-1159-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated -8.430 -> -7.880\n",
            "264911: done 614 games, reward -7.350, eps 0.02, speed 116.48 f/s, time 35.2 min\n",
            "264911: done 614 games, reward -7.350, eps 0.02, speed 116.48 f/s, time 35.2 min\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-7-20251125-1200-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated -7.880 -> -7.350\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-7-20251125-1200-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated -7.880 -> -7.350\n",
            "278067: done 630 games, reward -6.840, eps 0.02, speed 120.13 f/s, time 37.1 min\n",
            "278067: done 630 games, reward -6.840, eps 0.02, speed 120.13 f/s, time 37.1 min\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-6-20251125-1202-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated -7.350 -> -6.840\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-6-20251125-1202-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated -7.350 -> -6.840\n",
            "287188: done 640 games, reward -6.320, eps 0.02, speed 119.93 f/s, time 38.4 min\n",
            "287188: done 640 games, reward -6.320, eps 0.02, speed 119.93 f/s, time 38.4 min\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-6-20251125-1203-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated -6.840 -> -6.320\n",
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-6-20251125-1203-test_epsdec150000_rs10000_sync1000.dat\n",
            "Best reward updated -6.840 -> -6.320\n",
            "Stopping early at 300000 frames (MAX_FRAMES reached).\n",
            "Creating environment ALE/Pong-v5\n",
            "Stopping early at 300000 frames (MAX_FRAMES reached).\n",
            "Creating environment ALE/Pong-v5\n",
            "Saved gameplay video: videos/Double DQN/ALE_Pong-v5-final-20251125-1205.mp4 | Total reward: -14.00\n",
            "Saved gameplay video: videos/Double DQN/ALE_Pong-v5-final-20251125-1205.mp4 | Total reward: -14.00\n"
          ]
        }
      ],
      "source": [
        "# Training setup\n",
        "model_comment = (\n",
        "    f\"test_epsdec{config['EPSILON_DECAY_LAST_FRAME']}\"\n",
        "    f\"_rs{config['REPLAY_START_SIZE']}\"\n",
        "    f\"_sync{config['SYNC_TARGET_FRAMES']}\"\n",
        " )\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Environment and networks\n",
        "env = make_env(env_name)\n",
        "net = DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
        "tgt_net = DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
        "writer = SummaryWriter(comment=f\"-{env_name}-{model_comment}\")\n",
        "print(net)\n",
        "\n",
        "# Experience replay buffer and agent\n",
        "buffer = ExperienceBuffer(config[\"REPLAY_SIZE\"])\n",
        "agent = Agent(env, buffer)\n",
        "epsilon = config[\"EPSILON_START\"]\n",
        "\n",
        "# Optimizer setup\n",
        "optimizer = optim.Adam(net.parameters(), lr=config[\"LEARNING_RATE\"])\n",
        "total_rewards = []\n",
        "frame_idx = 0\n",
        "ts_frame = 0\n",
        "ts = time.time()\n",
        "best_m_reward = None\n",
        "\n",
        "set_double_dqn(True)  # Set to True to use Double DQN\n",
        "\n",
        "if USE_DOUBLE_DQN:\n",
        "    model_name = \"Double DQN\"\n",
        "else:\n",
        "    model_name = \"DQN\"\n",
        "\n",
        "performance_metric = {\n",
        "    \"Finished Games\": 0,\n",
        "    \"Reward\": 0.0,\n",
        "    \"Epsilon\": 0.0,\n",
        "    \"Speed (f/s)\": 0.0,\n",
        "    \"Elapsed Time (min)\": 0.0\n",
        "}\n",
        "\n",
        "# This creates a placeholder display area that we can update\n",
        "performance_display = display(Markdown(\"\"), display_id=True)\n",
        "\n",
        "# Training loop\n",
        "start_time = time.time()\n",
        "last_fps_log = start_time\n",
        "last_fps_frame = frame_idx\n",
        "FPS_LOG_INTERVAL = 5.0  # seconds\n",
        "MAX_FRAMES = 300_000  # cap to keep run under ~1–2 hours\n",
        "while True:\n",
        "    frame_idx += 1\n",
        "    if frame_idx >= MAX_FRAMES:\n",
        "        print(f\"Stopping early at {frame_idx} frames (MAX_FRAMES reached).\")\n",
        "\n",
        "        # Save final video before exit\n",
        "        timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
        "        video_filename = f\"videos/{model_name}/{safe_env_name}-final-{timestamp}.mp4\"\n",
        "        os.makedirs(f\"videos/{model_name}\", exist_ok=True)\n",
        "        record_policy_video(env_name, net, device, video_filename)\n",
        "        break\n",
        "\n",
        "    epsilon = max(config[\"EPSILON_FINAL\"], config[\"EPSILON_START\"] - frame_idx / config[\"EPSILON_DECAY_LAST_FRAME\"])\n",
        "\n",
        "    reward = agent.play_step(net, device, epsilon)\n",
        "    if reward is not None:\n",
        "        total_rewards.append(reward)\n",
        "        speed = (frame_idx - ts_frame) / (time.time() - ts)\n",
        "        elapsed = time.time() - start_time  # in seconds\n",
        "        ts_frame = frame_idx\n",
        "        ts = time.time()\n",
        "        m_reward = np.mean(total_rewards[-100:])\n",
        "        #  print(f\"{frame_idx}: done {len(total_rewards)} games, reward {m_reward:.3f}, \"\n",
        "        #      f\"eps {epsilon:.2f}, speed {speed:.2f} f/s, time {elapsed/60:.1f} min\")\n",
        "        writer.add_scalar(\"epsilon\", epsilon, frame_idx)\n",
        "        writer.add_scalar(\"speed\", speed, frame_idx)\n",
        "        writer.add_scalar(\"reward_100\", m_reward, frame_idx)\n",
        "        writer.add_scalar(\"reward\", reward, frame_idx)\n",
        "        if best_m_reward is None or m_reward > best_m_reward + config[\"SAVE_EPSILON\"]:\n",
        "            print(f\"{frame_idx}: done {len(total_rewards)} games, reward {m_reward:.3f}, \"\n",
        "                f\"eps {epsilon:.2f}, speed {speed:.2f} f/s, time {elapsed/60:.1f} min\")\n",
        "            \n",
        "            # Update performance metrics\n",
        "            performance_metric[\"Finished Games\"] = len(total_rewards)\n",
        "            performance_metric[\"Reward\"] = m_reward\n",
        "            performance_metric[\"Epsilon\"] = epsilon\n",
        "            performance_metric[\"Speed (f/s)\"] = speed\n",
        "            performance_metric[\"Elapsed Time (min)\"] = elapsed / 60.0\n",
        "\n",
        "            # Log model performance\n",
        "            log_model_performance(frame_idx, model_name, performance_metric)\n",
        "\n",
        "            # Save the model\n",
        "            timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
        "            model_filename = f\"{safe_env_name}-best_{int(m_reward)}-{timestamp}-{model_comment}.dat\"\n",
        "            model_path_local = os.path.join(save_dir_local, model_filename)\n",
        "\n",
        "            torch.save(net.state_dict(), model_path_local)\n",
        "\n",
        "            print(f\"Model saved to:\\n - Local:        {model_path_local}\")\n",
        "            if best_m_reward is not None:\n",
        "                print(f\"Best reward updated {best_m_reward:.3f} -> {m_reward:.3f}\")\n",
        "            best_m_reward = m_reward\n",
        "\n",
        "            # Record first video of training\n",
        "            if len(total_rewards) == 1:\n",
        "                video_filename = f\"videos/Pinball/{model_name}/{safe_env_name}-first-{int(m_reward)}-{timestamp}.mp4\"\n",
        "                os.makedirs(f\"videos/Pinball/{model_name}\", exist_ok=True)\n",
        "                record_policy_video(env_name, net, device, video_filename)\n",
        "            \n",
        "        if m_reward > config[\"MEAN_REWARD_BOUND\"]:\n",
        "            print(\"Solved in %d frames!\" % frame_idx)\n",
        "\n",
        "            # Final video recording before exit\n",
        "            timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
        "            video_filename = f\"videos/Pinball/{model_name}/{safe_env_name}-solved-{int(m_reward)}-{timestamp}.mp4\"\n",
        "            os.makedirs(f\"videos/Pinball/{model_name}\", exist_ok=True)\n",
        "            record_policy_video(env_name, net, device, video_filename)\n",
        "            break\n",
        "    if len(buffer) < config[\"REPLAY_START_SIZE\"]:\n",
        "        continue\n",
        "    if frame_idx % config[\"SYNC_TARGET_FRAMES\"] == 0:\n",
        "        tgt_net.load_state_dict(net.state_dict())\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    batch = buffer.sample(config[\"BATCH_SIZE\"])\n",
        "    loss_t = calc_loss(batch, net, tgt_net, device)\n",
        "    loss_t.backward()\n",
        "    optimizer.step()\n",
        "env.close()\n",
        "writer.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CWD: d:\\Github\\CSCI-166-Final-Project\n",
            "runs exists: True\n",
            "runs contents: ['Breakout', 'Nov24_07-20-24_DESKTOP-JV4U154-ALE', 'Nov24_07-55-37_DESKTOP-JV4U154-ALE', 'Nov24_08-52-01_DESKTOP-JV4U154-ALE', 'Nov24_09-37-01_DESKTOP-JV4U154-ALE', 'Nov24_10-58-12_DESKTOP-JV4U154-ALE', 'Nov24_12-00-34_DESKTOP-JV4U154-ALE', 'Nov24_15-08-17_DESKTOP-JV4U154-ALE', 'Nov24_15-57-07_DESKTOP-JV4U154-ALE', 'Nov24_21-33-49_DESKTOP-JV4U154-ALE', 'Nov24_22-10-56_DESKTOP-JV4U154-ALE', 'Nov24_22-59-59_DESKTOP-JV4U154-ALE', 'Nov24_23-57-56_DESKTOP-JV4U154-ALE', 'Nov25_05-18-13_DESKTOP-JV4U154-ALE', 'Nov25_05-56-22_DESKTOP-JV4U154-ALE', 'Nov25_09-22-07_DESKTOP-JV4U154-ALE', 'Nov25_10-00-08_DESKTOP-JV4U154-ALE', 'Nov25_10-47-02_DESKTOP-JV4U154-ALE', 'Nov25_11-25-11_DESKTOP-JV4U154-ALE', 'Nov25_13-53-05_DESKTOP-JV4U154-ALE', 'Nov25_14-33-13_DESKTOP-JV4U154-ALE', 'Nov25_15-16-05_DESKTOP-JV4U154-ALE', 'Nov25_16-04-24_DESKTOP-JV4U154-ALE', 'Nov25_16-54-19_DESKTOP-JV4U154-ALE', 'Nov25_17-34-30_DESKTOP-JV4U154-ALE', 'Nov25_18-17-26_DESKTOP-JV4U154-ALE', 'Nov25_18-57-43_DESKTOP-JV4U154-ALE', 'Nov25_19-40-28_DESKTOP-JV4U154-ALE', 'Nov25_20-21-11_DESKTOP-JV4U154-ALE', 'Nov25_21-22-13_DESKTOP-JV4U154-ALE', 'Nov25_22-38-38_DESKTOP-JV4U154-ALE', 'Nov25_23-50-52_DESKTOP-JV4U154-ALE', 'Nov26_00-30-50_DESKTOP-JV4U154-ALE', 'Nov26_01-13-30_DESKTOP-JV4U154-ALE', 'Nov26_01-53-54_DESKTOP-JV4U154-ALE', 'Nov26_02-36-24_DESKTOP-JV4U154-ALE', 'Nov26_03-16-26_DESKTOP-JV4U154-ALE', 'Pong']\n",
            " - Breakout | isdir: True\n",
            "   inner: []\n",
            " - Nov24_07-20-24_DESKTOP-JV4U154-ALE | isdir: True\n",
            "   inner: ['Pong-v5-test_epsdec150000_rs10000_sync1000']\n",
            " - Nov24_07-55-37_DESKTOP-JV4U154-ALE | isdir: True\n",
            "   inner: ['Pong-v5-test_epsdec150000_rs10000_sync1000']\n",
            " - Nov24_08-52-01_DESKTOP-JV4U154-ALE | isdir: True\n",
            "   inner: ['Pong-v5-test_epsdec150000_rs10000_sync1000']\n",
            " - Nov24_09-37-01_DESKTOP-JV4U154-ALE | isdir: True\n",
            "   inner: ['Pong-v5-test_epsdec150000_rs10000_sync1000']\n",
            " - Nov24_10-58-12_DESKTOP-JV4U154-ALE | isdir: True\n",
            "   inner: ['Pong-v5-test_epsdec150000_rs10000_sync1000']\n",
            " - Nov24_12-00-34_DESKTOP-JV4U154-ALE | isdir: True\n",
            "   inner: ['Pong-v5-test_epsdec150000_rs10000_sync1000']\n",
            " - Nov24_15-08-17_DESKTOP-JV4U154-ALE | isdir: True\n",
            "   inner: ['Pong-v5-test_epsdec150000_rs10000_sync1000']\n",
            " - Nov24_15-57-07_DESKTOP-JV4U154-ALE | isdir: True\n",
            "   inner: ['Pong-v5-test_epsdec150000_rs10000_sync1000']\n",
            " - Nov24_21-33-49_DESKTOP-JV4U154-ALE | isdir: True\n",
            "   inner: ['Pong-v5-test_epsdec150000_rs10000_sync1000']\n",
            " - Nov24_22-10-56_DESKTOP-JV4U154-ALE | isdir: True\n",
            "   inner: ['Pong-v5-test_epsdec150000_rs10000_sync1000']\n",
            " - Nov24_22-59-59_DESKTOP-JV4U154-ALE | isdir: True\n",
            "   inner: ['Pong-v5-test_epsdec150000_rs10000_sync500']\n",
            " - Nov24_23-57-56_DESKTOP-JV4U154-ALE | isdir: True\n",
            "   inner: ['Pong-v5-test_epsdec150000_rs10000_sync500']\n",
            " - Nov25_05-18-13_DESKTOP-JV4U154-ALE | isdir: True\n",
            "   inner: ['Pong-v5-test_epsdec150000_rs10000_sync1500']\n",
            " - Nov25_05-56-22_DESKTOP-JV4U154-ALE | isdir: True\n",
            "   inner: ['Pong-v5-test_epsdec150000_rs10000_sync1500']\n",
            " - Nov25_09-22-07_DESKTOP-JV4U154-ALE | isdir: True\n",
            "   inner: ['Pong-v5-test_epsdec150000_rs10000_sync1000']\n",
            " - Nov25_10-00-08_DESKTOP-JV4U154-ALE | isdir: True\n",
            "   inner: ['Pong-v5-test_epsdec150000_rs10000_sync1000']\n",
            " - Nov25_10-47-02_DESKTOP-JV4U154-ALE | isdir: True\n",
            "   inner: ['Pong-v5-test_epsdec150000_rs10000_sync1000']\n",
            " - Nov25_11-25-11_DESKTOP-JV4U154-ALE | isdir: True\n",
            "   inner: ['Pong-v5-test_epsdec150000_rs10000_sync1000']\n",
            " - Nov25_13-53-05_DESKTOP-JV4U154-ALE | isdir: True\n",
            "   inner: ['Breakout-v5-test_epsdec150000_rs10000_sync1000']\n",
            " - Nov25_14-33-13_DESKTOP-JV4U154-ALE | isdir: True\n",
            "   inner: ['Breakout-v5-test_epsdec150000_rs10000_sync1000']\n",
            " - Nov25_15-16-05_DESKTOP-JV4U154-ALE | isdir: True\n",
            "   inner: ['Breakout-v5-test_epsdec150000_rs10000_sync1000']\n",
            " - Nov25_16-04-24_DESKTOP-JV4U154-ALE | isdir: True\n",
            "   inner: ['Breakout-v5-test_epsdec150000_rs10000_sync1000']\n",
            " - Nov25_16-54-19_DESKTOP-JV4U154-ALE | isdir: True\n",
            "   inner: ['Breakout-v5-test_epsdec150000_rs10000_sync1000']\n",
            " - Nov25_17-34-30_DESKTOP-JV4U154-ALE | isdir: True\n",
            "   inner: ['Breakout-v5-test_epsdec150000_rs10000_sync1000']\n",
            " - Nov25_18-17-26_DESKTOP-JV4U154-ALE | isdir: True\n",
            "   inner: ['Breakout-v5-test_epsdec150000_rs10000_sync1000']\n",
            " - Nov25_18-57-43_DESKTOP-JV4U154-ALE | isdir: True\n",
            "   inner: ['Breakout-v5-test_epsdec150000_rs10000_sync1000']\n",
            " - Nov25_19-40-28_DESKTOP-JV4U154-ALE | isdir: True\n",
            "   inner: ['Breakout-v5-test_epsdec150000_rs10000_sync1000']\n",
            " - Nov25_20-21-11_DESKTOP-JV4U154-ALE | isdir: True\n",
            "   inner: ['Breakout-v5-test_epsdec150000_rs10000_sync1000']\n",
            " - Nov25_21-22-13_DESKTOP-JV4U154-ALE | isdir: True\n",
            "   inner: ['Breakout-v5-test_epsdec150000_rs10000_sync500']\n",
            " - Nov25_22-38-38_DESKTOP-JV4U154-ALE | isdir: True\n",
            "   inner: ['Breakout-v5-test_epsdec150000_rs10000_sync500']\n",
            " - Nov25_23-50-52_DESKTOP-JV4U154-ALE | isdir: True\n",
            "   inner: ['Breakout-v5-test_epsdec150000_rs10000_sync1500']\n",
            " - Nov26_00-30-50_DESKTOP-JV4U154-ALE | isdir: True\n",
            "   inner: ['Breakout-v5-test_epsdec150000_rs10000_sync1500']\n",
            " - Nov26_01-13-30_DESKTOP-JV4U154-ALE | isdir: True\n",
            "   inner: ['Breakout-v5-test_epsdec150000_rs10000_sync1000']\n",
            " - Nov26_01-53-54_DESKTOP-JV4U154-ALE | isdir: True\n",
            "   inner: ['Breakout-v5-test_epsdec150000_rs10000_sync1000']\n",
            " - Nov26_02-36-24_DESKTOP-JV4U154-ALE | isdir: True\n",
            "   inner: ['Breakout-v5-test_epsdec150000_rs10000_sync1000']\n",
            " - Nov26_03-16-26_DESKTOP-JV4U154-ALE | isdir: True\n",
            "   inner: ['Breakout-v5-test_epsdec150000_rs10000_sync1000']\n",
            " - Pong | isdir: True\n",
            "   inner: []\n"
          ]
        }
      ],
      "source": [
        "import os, pathlib\n",
        "print(\"CWD:\", os.getcwd())\n",
        "print(\"runs exists:\", os.path.isdir(\"runs\"))\n",
        "if os.path.isdir(\"runs\"):\n",
        "    print(\"runs contents:\", os.listdir(\"runs\"))\n",
        "    for name in os.listdir(\"runs\"):\n",
        "        p = os.path.join(\"runs\", name)\n",
        "        print(\" -\", name, \"| isdir:\", os.path.isdir(p))\n",
        "        if os.path.isdir(p):\n",
        "            # list one level deeper\n",
        "            inner = os.listdir(p)\n",
        "            print(\"   inner:\", inner[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 195,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Nov24_07-20-24_DESKTOP-JV4U154-ALE {'images': [], 'audio': [], 'histograms': [], 'scalars': [], 'distributions': [], 'tensors': [], 'graph': False, 'meta_graph': False, 'run_metadata': []}\n",
            "Nov24_07-55-37_DESKTOP-JV4U154-ALE {'images': [], 'audio': [], 'histograms': [], 'scalars': [], 'distributions': [], 'tensors': [], 'graph': False, 'meta_graph': False, 'run_metadata': []}\n",
            "Nov24_08-52-01_DESKTOP-JV4U154-ALE {'images': [], 'audio': [], 'histograms': [], 'scalars': [], 'distributions': [], 'tensors': [], 'graph': False, 'meta_graph': False, 'run_metadata': []}\n",
            "Nov24_09-37-01_DESKTOP-JV4U154-ALE {'images': [], 'audio': [], 'histograms': [], 'scalars': [], 'distributions': [], 'tensors': [], 'graph': False, 'meta_graph': False, 'run_metadata': []}\n",
            "Nov24_10-58-12_DESKTOP-JV4U154-ALE {'images': [], 'audio': [], 'histograms': [], 'scalars': [], 'distributions': [], 'tensors': [], 'graph': False, 'meta_graph': False, 'run_metadata': []}\n",
            "Nov24_12-00-34_DESKTOP-JV4U154-ALE {'images': [], 'audio': [], 'histograms': [], 'scalars': [], 'distributions': [], 'tensors': [], 'graph': False, 'meta_graph': False, 'run_metadata': []}\n",
            "Nov24_15-08-17_DESKTOP-JV4U154-ALE {'images': [], 'audio': [], 'histograms': [], 'scalars': [], 'distributions': [], 'tensors': [], 'graph': False, 'meta_graph': False, 'run_metadata': []}\n",
            "Nov24_15-57-07_DESKTOP-JV4U154-ALE {'images': [], 'audio': [], 'histograms': [], 'scalars': [], 'distributions': [], 'tensors': [], 'graph': False, 'meta_graph': False, 'run_metadata': []}\n",
            "Nov24_21-33-49_DESKTOP-JV4U154-ALE {'images': [], 'audio': [], 'histograms': [], 'scalars': [], 'distributions': [], 'tensors': [], 'graph': False, 'meta_graph': False, 'run_metadata': []}\n",
            "Nov24_22-10-56_DESKTOP-JV4U154-ALE {'images': [], 'audio': [], 'histograms': [], 'scalars': [], 'distributions': [], 'tensors': [], 'graph': False, 'meta_graph': False, 'run_metadata': []}\n",
            "Nov24_22-59-59_DESKTOP-JV4U154-ALE {'images': [], 'audio': [], 'histograms': [], 'scalars': [], 'distributions': [], 'tensors': [], 'graph': False, 'meta_graph': False, 'run_metadata': []}\n",
            "Nov24_23-57-56_DESKTOP-JV4U154-ALE {'images': [], 'audio': [], 'histograms': [], 'scalars': [], 'distributions': [], 'tensors': [], 'graph': False, 'meta_graph': False, 'run_metadata': []}\n",
            "Nov25_05-18-13_DESKTOP-JV4U154-ALE {'images': [], 'audio': [], 'histograms': [], 'scalars': [], 'distributions': [], 'tensors': [], 'graph': False, 'meta_graph': False, 'run_metadata': []}\n",
            "Nov25_05-56-22_DESKTOP-JV4U154-ALE {'images': [], 'audio': [], 'histograms': [], 'scalars': [], 'distributions': [], 'tensors': [], 'graph': False, 'meta_graph': False, 'run_metadata': []}\n",
            "Nov25_09-22-07_DESKTOP-JV4U154-ALE {'images': [], 'audio': [], 'histograms': [], 'scalars': [], 'distributions': [], 'tensors': [], 'graph': False, 'meta_graph': False, 'run_metadata': []}\n",
            "Nov25_10-00-08_DESKTOP-JV4U154-ALE {'images': [], 'audio': [], 'histograms': [], 'scalars': [], 'distributions': [], 'tensors': [], 'graph': False, 'meta_graph': False, 'run_metadata': []}\n",
            "Nov25_10-47-02_DESKTOP-JV4U154-ALE {'images': [], 'audio': [], 'histograms': [], 'scalars': [], 'distributions': [], 'tensors': [], 'graph': False, 'meta_graph': False, 'run_metadata': []}\n",
            "Nov25_11-25-11_DESKTOP-JV4U154-ALE {'images': [], 'audio': [], 'histograms': [], 'scalars': [], 'distributions': [], 'tensors': [], 'graph': False, 'meta_graph': False, 'run_metadata': []}\n",
            "Nov25_13-53-05_DESKTOP-JV4U154-ALE {'images': [], 'audio': [], 'histograms': [], 'scalars': [], 'distributions': [], 'tensors': [], 'graph': False, 'meta_graph': False, 'run_metadata': []}\n",
            "Nov25_14-33-13_DESKTOP-JV4U154-ALE {'images': [], 'audio': [], 'histograms': [], 'scalars': [], 'distributions': [], 'tensors': [], 'graph': False, 'meta_graph': False, 'run_metadata': []}\n",
            "Nov25_15-16-05_DESKTOP-JV4U154-ALE {'images': [], 'audio': [], 'histograms': [], 'scalars': [], 'distributions': [], 'tensors': [], 'graph': False, 'meta_graph': False, 'run_metadata': []}\n",
            "Nov25_16-04-24_DESKTOP-JV4U154-ALE {'images': [], 'audio': [], 'histograms': [], 'scalars': [], 'distributions': [], 'tensors': [], 'graph': False, 'meta_graph': False, 'run_metadata': []}\n",
            "Nov25_16-54-19_DESKTOP-JV4U154-ALE {'images': [], 'audio': [], 'histograms': [], 'scalars': [], 'distributions': [], 'tensors': [], 'graph': False, 'meta_graph': False, 'run_metadata': []}\n",
            "Nov25_17-34-30_DESKTOP-JV4U154-ALE {'images': [], 'audio': [], 'histograms': [], 'scalars': [], 'distributions': [], 'tensors': [], 'graph': False, 'meta_graph': False, 'run_metadata': []}\n",
            "Nov25_18-17-26_DESKTOP-JV4U154-ALE {'images': [], 'audio': [], 'histograms': [], 'scalars': [], 'distributions': [], 'tensors': [], 'graph': False, 'meta_graph': False, 'run_metadata': []}\n",
            "Nov25_18-57-43_DESKTOP-JV4U154-ALE {'images': [], 'audio': [], 'histograms': [], 'scalars': [], 'distributions': [], 'tensors': [], 'graph': False, 'meta_graph': False, 'run_metadata': []}\n",
            "Nov25_19-40-28_DESKTOP-JV4U154-ALE {'images': [], 'audio': [], 'histograms': [], 'scalars': [], 'distributions': [], 'tensors': [], 'graph': False, 'meta_graph': False, 'run_metadata': []}\n",
            "Nov25_20-21-11_DESKTOP-JV4U154-ALE {'images': [], 'audio': [], 'histograms': [], 'scalars': [], 'distributions': [], 'tensors': [], 'graph': False, 'meta_graph': False, 'run_metadata': []}\n",
            "Nov25_21-22-13_DESKTOP-JV4U154-ALE {'images': [], 'audio': [], 'histograms': [], 'scalars': [], 'distributions': [], 'tensors': [], 'graph': False, 'meta_graph': False, 'run_metadata': []}\n",
            "Nov25_22-38-38_DESKTOP-JV4U154-ALE {'images': [], 'audio': [], 'histograms': [], 'scalars': [], 'distributions': [], 'tensors': [], 'graph': False, 'meta_graph': False, 'run_metadata': []}\n",
            "Nov25_23-50-52_DESKTOP-JV4U154-ALE {'images': [], 'audio': [], 'histograms': [], 'scalars': [], 'distributions': [], 'tensors': [], 'graph': False, 'meta_graph': False, 'run_metadata': []}\n",
            "Nov26_00-30-50_DESKTOP-JV4U154-ALE {'images': [], 'audio': [], 'histograms': [], 'scalars': [], 'distributions': [], 'tensors': [], 'graph': False, 'meta_graph': False, 'run_metadata': []}\n",
            "Nov26_01-13-30_DESKTOP-JV4U154-ALE {'images': [], 'audio': [], 'histograms': [], 'scalars': [], 'distributions': [], 'tensors': [], 'graph': False, 'meta_graph': False, 'run_metadata': []}\n",
            "Nov26_01-53-54_DESKTOP-JV4U154-ALE {'images': [], 'audio': [], 'histograms': [], 'scalars': [], 'distributions': [], 'tensors': [], 'graph': False, 'meta_graph': False, 'run_metadata': []}\n",
            "Nov26_02-36-24_DESKTOP-JV4U154-ALE {'images': [], 'audio': [], 'histograms': [], 'scalars': [], 'distributions': [], 'tensors': [], 'graph': False, 'meta_graph': False, 'run_metadata': []}\n",
            "Nov26_03-16-26_DESKTOP-JV4U154-ALE {'images': [], 'audio': [], 'histograms': [], 'scalars': [], 'distributions': [], 'tensors': [], 'graph': False, 'meta_graph': False, 'run_metadata': []}\n",
            "Done. Now point TensorBoard at 'runs_norm'.\n"
          ]
        }
      ],
      "source": [
        "from tensorboard.backend.event_processing import event_accumulator\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import os\n",
        "\n",
        "# Configure env-specific normalization for rewards\n",
        "ENV_INFO = {\n",
        "    \"Breakout\": {\"min\": 0.0, \"max\": 400.0},\n",
        "    \"Pong\": {\"min\": -21.0, \"max\": 21.0},\n",
        "}\n",
        "\n",
        "def norm_reward(env, r):\n",
        "    info = ENV_INFO[env]\n",
        "    return (r - info[\"min\"]) / (info[\"max\"] - info[\"min\"])\n",
        "\n",
        "def norm_01(x, x_min, x_max):\n",
        "    if x_max == x_min:\n",
        "        return 0.0\n",
        "    return (x - x_min) / (x_max - x_min)\n",
        "\n",
        "src_root = \"runs\"\n",
        "dst_root = \"runs_norm\"  # new logs go here\n",
        "\n",
        "os.makedirs(dst_root, exist_ok=True)\n",
        "\n",
        "for run_name in os.listdir(src_root):\n",
        "    run_path = os.path.join(src_root, run_name)\n",
        "    if not os.path.isdir(run_path):\n",
        "        continue\n",
        "\n",
        "    # Simple heuristic: detect env from run_name\n",
        "    env = \"Pong\" if (\"Pong\" in run_name or \"pong\" in run_name) else \"Breakout\"\n",
        "\n",
        "    ea = event_accumulator.EventAccumulator(run_path)\n",
        "    try:\n",
        "        ea.Reload()\n",
        "    except Exception as e:\n",
        "        print(\"Failed to load\", run_name, \":\", e)\n",
        "        continue\n",
        "\n",
        "    print(run_name, ea.Tags())\n",
        "\n",
        "    scalar_tags = ea.Tags().get(\"scalars\", [])\n",
        "    if not scalar_tags:\n",
        "        continue\n",
        "\n",
        "    writer = SummaryWriter(log_dir=os.path.join(dst_root, run_name))\n",
        "\n",
        "    # ----- normalize reward_100 and reward if present -----\n",
        "    if \"reward_100\" in scalar_tags:\n",
        "        for ev in ea.Scalars(\"reward_100\"):\n",
        "            r_norm = norm_reward(env, ev.value)\n",
        "            writer.add_scalar(\"reward_100_norm\", r_norm, ev.step)\n",
        "\n",
        "    if \"reward\" in scalar_tags:\n",
        "        for ev in ea.Scalars(\"reward\"):\n",
        "            r_norm = norm_reward(env, ev.value)\n",
        "            writer.add_scalar(\"reward_norm\", r_norm, ev.step)\n",
        "\n",
        "    # ----- normalize epsilon to [0,1] if present -----\n",
        "    if \"epsilon\" in scalar_tags:\n",
        "        eps_vals = ea.Scalars(\"epsilon\")\n",
        "        eps_min = min(ev.value for ev in eps_vals)\n",
        "        eps_max = max(ev.value for ev in eps_vals)\n",
        "        for ev in eps_vals:\n",
        "            e_norm = norm_01(ev.value, eps_min, eps_max)\n",
        "            writer.add_scalar(\"epsilon_norm\", e_norm, ev.step)\n",
        "\n",
        "    # ----- normalize speed per run if present -----\n",
        "    if \"speed\" in scalar_tags:\n",
        "        sp_vals = ea.Scalars(\"speed\")\n",
        "        sp_min = min(ev.value for ev in sp_vals)\n",
        "        sp_max = max(ev.value for ev in sp_vals)\n",
        "        for ev in sp_vals:\n",
        "            s_norm = norm_01(ev.value, sp_min, sp_max)\n",
        "            writer.add_scalar(\"speed_norm\", s_norm, ev.step)\n",
        "\n",
        "    writer.close()\n",
        "    print(\"Processed\", run_name)\n",
        "\n",
        "print(\"Done. Now point TensorBoard at 'runs_norm'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[TensorBoardInfo(version='2.13.0', start_time=1764166536, pid=13180, port=6006, path_prefix='', logdir='runs/Pong', db='', cache_key='eyJhcmd1bWVudHMiOlsiLS1sb2dkaXIiLCJydW5zL1BvbmciXSwiY29uZmlndXJlX2t3YXJncyI6e30sIndvcmtpbmdfZGlyZWN0b3J5IjoiZDpcXEdpdGh1YlxcQ1NDSS0xNjYtRmluYWwtUHJvamVjdCJ9'),\n",
              " TensorBoardInfo(version='2.13.0', start_time=1764168877, pid=17416, port=6006, path_prefix='', logdir='runs_norm', db='', cache_key='eyJhcmd1bWVudHMiOlsiLS1sb2dkaXIiLCJydW5zX25vcm0iLCItLXBvcnQiLCI2MDA2Il0sImNvbmZpZ3VyZV9rd2FyZ3MiOnt9LCJ3b3JraW5nX2RpcmVjdG9yeSI6ImQ6XFxHaXRodWJcXENTQ0ktMTY2LUZpbmFsLVByb2plY3QifQ=='),\n",
              " TensorBoardInfo(version='2.13.0', start_time=1764166976, pid=26936, port=6006, path_prefix='', logdir='runs', db='', cache_key='eyJhcmd1bWVudHMiOlsiLS1sb2dkaXIiLCJydW5zIiwiLS1wb3J0IiwiNjAwNiJdLCJjb25maWd1cmVfa3dhcmdzIjp7fSwid29ya2luZ19kaXJlY3RvcnkiOiJkOlxcR2l0aHViXFxDU0NJLTE2Ni1GaW5hbC1Qcm9qZWN0In0='),\n",
              " TensorBoardInfo(version='2.13.0', start_time=1764166433, pid=27732, port=6006, path_prefix='', logdir='runs/Pong/', db='', cache_key='eyJhcmd1bWVudHMiOlsiLS1sb2dkaXIiLCJydW5zL1BvbmcvIl0sImNvbmZpZ3VyZV9rd2FyZ3MiOnt9LCJ3b3JraW5nX2RpcmVjdG9yeSI6ImQ6XFxHaXRodWJcXENTQ0ktMTY2LUZpbmFsLVByb2plY3QifQ=='),\n",
              " TensorBoardInfo(version='2.13.0', start_time=1763954901, pid=29556, port=6006, path_prefix='', logdir='runs', db='', cache_key='eyJhcmd1bWVudHMiOlsiLS1sb2dkaXIiLCJydW5zIl0sImNvbmZpZ3VyZV9rd2FyZ3MiOnt9LCJ3b3JraW5nX2RpcmVjdG9yeSI6ImQ6XFxHaXRodWJcXENTQ0ktMTY2LUZpbmFsLVByb2plY3QifQ==')]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%reload_ext tensorboard\n",
        "\n",
        "from tensorboard import manager\n",
        "manager.get_all()   # show stale instances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: The process \"29556\" not found.\n"
          ]
        }
      ],
      "source": [
        "!taskkill /F /PID 29556\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "%reload_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Reusing TensorBoard on port 6007 (pid 19836), started 0:00:19 ago. (Use '!kill 19836' to kill it.)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "      <iframe id=\"tensorboard-frame-814a498f96f91ded\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
              "      </iframe>\n",
              "      <script>\n",
              "        (function() {\n",
              "          const frame = document.getElementById(\"tensorboard-frame-814a498f96f91ded\");\n",
              "          const url = new URL(\"http://localhost\");\n",
              "          const port = 6007;\n",
              "          if (port) {\n",
              "            url.port = port;\n",
              "          }\n",
              "          frame.src = url;\n",
              "        })();\n",
              "      </script>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir runs --port 6007\n",
        "\n",
        "%reload_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: The process \"tensorboard.exe\" not found.\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "!taskkill /F /IM tensorboard.exe"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
