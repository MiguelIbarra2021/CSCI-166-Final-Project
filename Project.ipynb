{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtOuIKifmmyk"
      },
      "source": [
        "# Test 3 --- Final Test for 06.11.2025"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 250,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqL9333gml9l",
        "outputId": "70c32a3d-216c-4e64-ebaf-1301f39612b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: tensorboard in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (2.20.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from tensorboard) (2.3.1)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from tensorboard) (1.76.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from tensorboard) (3.10)\n",
            "Requirement already satisfied: numpy>=1.12.0 in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from tensorboard) (2.2.6)\n",
            "Requirement already satisfied: packaging in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from tensorboard) (25.0)\n",
            "Requirement already satisfied: pillow in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from tensorboard) (12.0.0)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from tensorboard) (6.33.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from tensorboard) (80.9.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from tensorboard) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from tensorboard) (3.1.3)\n",
            "Requirement already satisfied: typing-extensions~=4.12 in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from grpcio>=1.48.2->tensorboard) (4.15.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from werkzeug>=1.0.1->tensorboard) (3.0.3)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.2 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: tabulate in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (0.9.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.2 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: gymnasium[accept-rom-license,atari] in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from gymnasium[accept-rom-license,atari]) (2.2.6)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from gymnasium[accept-rom-license,atari]) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from gymnasium[accept-rom-license,atari]) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from gymnasium[accept-rom-license,atari]) (0.0.4)\n",
            "Requirement already satisfied: ale_py>=0.9 in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from gymnasium[accept-rom-license,atari]) (0.11.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: gymnasium 1.2.2 does not provide the extra 'accept-rom-license'\n",
            "\n",
            "[notice] A new release of pip is available: 25.2 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: autorom in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (0.6.1)\n",
            "Requirement already satisfied: click in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from autorom) (8.3.0)\n",
            "Requirement already satisfied: requests in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from autorom) (2.32.5)\n",
            "Requirement already satisfied: colorama in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from click->autorom) (0.4.6)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from requests->autorom) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from requests->autorom) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from requests->autorom) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from requests->autorom) (2025.10.5)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.2 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: stable-baselines3 in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (2.7.0)\n",
            "Requirement already satisfied: gymnasium<1.3.0,>=0.29.1 in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from stable-baselines3) (1.2.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.20 in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from stable-baselines3) (2.2.6)\n",
            "Requirement already satisfied: torch<3.0,>=2.3 in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from stable-baselines3) (2.9.0)\n",
            "Requirement already satisfied: cloudpickle in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from stable-baselines3) (3.1.2)\n",
            "Requirement already satisfied: pandas in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from stable-baselines3) (2.3.3)\n",
            "Requirement already satisfied: matplotlib in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from stable-baselines3) (3.10.7)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from gymnasium<1.3.0,>=0.29.1->stable-baselines3) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from gymnasium<1.3.0,>=0.29.1->stable-baselines3) (0.0.4)\n",
            "Requirement already satisfied: filelock in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from torch<3.0,>=2.3->stable-baselines3) (3.20.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from torch<3.0,>=2.3->stable-baselines3) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from torch<3.0,>=2.3->stable-baselines3) (3.5)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from torch<3.0,>=2.3->stable-baselines3) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from torch<3.0,>=2.3->stable-baselines3) (2025.10.0)\n",
            "Requirement already satisfied: setuptools in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from torch<3.0,>=2.3->stable-baselines3) (80.9.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from sympy>=1.13.3->torch<3.0,>=2.3->stable-baselines3) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from jinja2->torch<3.0,>=2.3->stable-baselines3) (3.0.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib->stable-baselines3) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib->stable-baselines3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib->stable-baselines3) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib->stable-baselines3) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib->stable-baselines3) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib->stable-baselines3) (12.0.0)\n",
            "Requirement already satisfied: pyparsing>=3 in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib->stable-baselines3) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib->stable-baselines3) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3) (1.17.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from pandas->stable-baselines3) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from pandas->stable-baselines3) (2025.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.2 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: imageio[ffmpeg] in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (2.37.2)\n",
            "Requirement already satisfied: numpy in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from imageio[ffmpeg]) (2.2.6)\n",
            "Requirement already satisfied: pillow>=8.3.2 in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from imageio[ffmpeg]) (12.0.0)\n",
            "Requirement already satisfied: imageio-ffmpeg in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from imageio[ffmpeg]) (0.6.0)\n",
            "Requirement already satisfied: psutil in c:\\users\\migue\\appdata\\roaming\\python\\python313\\site-packages (from imageio[ffmpeg]) (7.1.3)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.2 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "# Install TensorBoard for visualization\n",
        "%pip install tensorboard\n",
        "%pip install tabulate\n",
        "\n",
        "# Install required packages for reinforcement learning with Atari environments\n",
        "%pip install gymnasium[atari,accept-rom-license]\n",
        "%pip install autorom\n",
        "%pip install stable-baselines3\n",
        "\n",
        "# Install imageio for video recording\n",
        "%pip install imageio[ffmpeg]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 251,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oi2qmVoVmxve",
        "outputId": "8d9a366d-9bc7-4ae0-dfaf-2c73db288b7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AutoROM will download the Atari 2600 ROMs.\n",
            "They will be installed to:\n",
            "\tC:\\Users\\migue\\AppData\\Roaming\\Python\\Python313\\site-packages\\AutoROM\\roms\n",
            "\n",
            "Existing ROMs will be overwritten.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\migue\\AppData\\Roaming\\Python\\Python313\\site-packages\\AutoROM\\AutoROM.py:264: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
            "  import pkg_resources\n"
          ]
        }
      ],
      "source": [
        "# Download Atari ROMs\n",
        "!AutoROM --accept-license"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQyqgMFzT-qi"
      },
      "source": [
        "# Install the Gym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 252,
      "metadata": {
        "id": "_W_afhrzUAnp"
      },
      "outputs": [],
      "source": [
        "import ale_py\n",
        "import gymnasium as gym"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0n7zQrALq7M"
      },
      "source": [
        "# Configure the model save folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 253,
      "metadata": {
        "id": "vlWPUjKfLv9y"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "save_dir = \"models/\"\n",
        "os.makedirs(save_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAEvyoukL1T3"
      },
      "source": [
        "# Now Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 254,
      "metadata": {
        "id": "VCbjkLLSxCUN"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "import argparse\n",
        "import time\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import collections\n",
        "import typing as tt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.utils.tensorboard.writer import SummaryWriter\n",
        "\n",
        "import imageio\n",
        "from IPython.display import HTML, display\n",
        "from base64 import b64encode\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"imageio\")\n",
        "os.environ[\"IMAGEIO_FFMPEG_LOGLEVEL\"] = \"quiet\"  # or \"quiet\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 255,
      "metadata": {
        "id": "dIJ32Rs6xJsV"
      },
      "outputs": [],
      "source": [
        "#dqn_model\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, input_shape, n_actions):\n",
        "        super(DQN, self).__init__()\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "        )\n",
        "        size = self.conv(torch.zeros(1, *input_shape)).size()[-1]\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, n_actions)\n",
        "        )\n",
        "    def forward(self, x: torch.ByteTensor):\n",
        "        x = x.float() / 255.0\n",
        "        return self.fc(self.conv(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 256,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vk2CtGcOBcQo",
        "outputId": "84795c85-6ace-4c26-90ea-9b73fc4766d3"
      },
      "outputs": [],
      "source": [
        "#wrappers\n",
        "\n",
        "from gymnasium import spaces\n",
        "from stable_baselines3.common import atari_wrappers\n",
        "\n",
        "\n",
        "class ImageToPyTorch(gym.ObservationWrapper):\n",
        "    \"\"\"\n",
        "    ImageToPyTorch: Reorders image dimensions from (H, W, C) to (C, H, W)\n",
        "    for compatibility with PyTorch convolutional layers.\n",
        "    \"\"\"\n",
        "    def __init__(self, env):\n",
        "        super(ImageToPyTorch, self).__init__(env)\n",
        "        obs = self.observation_space\n",
        "        assert isinstance(obs, gym.spaces.Box)\n",
        "        assert len(obs.shape) == 3\n",
        "        new_shape = (obs.shape[-1], obs.shape[0], obs.shape[1])\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=obs.low.min(), high=obs.high.max(),\n",
        "            shape=new_shape, dtype=obs.dtype)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        return np.moveaxis(observation, 2, 0)\n",
        "\n",
        "\n",
        "class BufferWrapper(gym.ObservationWrapper):\n",
        "    \"\"\"\n",
        "    BufferWrapper: Maintains a rolling window of the last `n_steps` frames\n",
        "    to give the agent a sense of temporal context.\n",
        "    \"\"\"\n",
        "    def __init__(self, env, n_steps):\n",
        "        super(BufferWrapper, self).__init__(env)\n",
        "        obs = env.observation_space\n",
        "        assert isinstance(obs, spaces.Box)\n",
        "        new_obs = gym.spaces.Box(\n",
        "            obs.low.repeat(n_steps, axis=0), obs.high.repeat(n_steps, axis=0),\n",
        "            dtype=obs.dtype)\n",
        "        self.observation_space = new_obs\n",
        "        self.buffer = collections.deque(maxlen=n_steps)\n",
        "\n",
        "    def reset(self, *, seed: tt.Optional[int] = None, options: tt.Optional[dict[str, tt.Any]] = None):\n",
        "        for _ in range(self.buffer.maxlen):\n",
        "            self.buffer.append(np.zeros_like(self.env.observation_space.low))\n",
        "        obs, extra = self.env.reset()\n",
        "        return self.observation(obs), extra\n",
        "\n",
        "    def observation(self, observation: np.ndarray) -> np.ndarray:\n",
        "        self.buffer.append(observation)\n",
        "        return np.concatenate(self.buffer)\n",
        "\n",
        "\n",
        "def make_env(env_name: str, n_steps=4, render_mode=None, **kwargs):\n",
        "    print(f\"Creating environment {env_name}\")\n",
        "    env = gym.make(env_name, render_mode=render_mode, **kwargs)\n",
        "    env = atari_wrappers.AtariWrapper(env, clip_reward=False, noop_max=0)\n",
        "    env = ImageToPyTorch(env)\n",
        "    env = BufferWrapper(env, n_steps=n_steps)\n",
        "    return env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Only rerun if it hasn't been run before; Prevents resetting the experiment log\n",
        "\n",
        "has_run = 'config_history' in globals() and 'version_counter' in globals()\n",
        "\n",
        "# Experiment logging utility\n",
        "from IPython.display import Markdown, clear_output, display\n",
        "import pandas as pd\n",
        "import datetime\n",
        "\n",
        "# Persistent experiment history\n",
        "if not has_run:\n",
        "    config_history = []\n",
        "    version_counter = 0\n",
        "\n",
        "# Log experiment configurations\n",
        "def log_experiment(config_dict, note=\"\"):\n",
        "    \"\"\"Log experiment configuration and display the table.\"\"\"\n",
        "    global version_counter, config_history\n",
        "\n",
        "    \"\"\"Append config to table and display nicely. Only if it's new.\"\"\"\n",
        "    if(config_history and all(config_dict.get(k) == config_history[-1].get(k) for k in config_dict)):\n",
        "        return  # Skip logging if the config is the same as the last one\n",
        "\n",
        "    entry = {\n",
        "        \"version\": version_counter,\n",
        "        \"timestamp\": datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "        **config_dict,\n",
        "        \"note\": note\n",
        "    }\n",
        "    # Append the new entry to the history\n",
        "    config_history.append(entry)\n",
        "    version_counter += 1\n",
        "\n",
        "    # Display the updated experiment log\n",
        "    clear_output(wait=True)\n",
        "    df = pd.DataFrame(config_history)\n",
        "    display(Markdown(df.to_markdown(index=False)))\n",
        "\n",
        "has_run = 'performance_history' in globals()\n",
        "\n",
        "if not has_run:\n",
        "    performance_history = []\n",
        "\n",
        "def log_model_performance(frame_index, model_name, performance_metric_dict):\n",
        "    \"\"\"Log model performance with a note.\"\"\"\n",
        "    global performance_history\n",
        "\n",
        "    entry = {\n",
        "        \"Frame Index\": frame_index,\n",
        "        \"model_name\": model_name,\n",
        "        **performance_metric_dict,\n",
        "        \"timestamp\": datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    }\n",
        "    # Append the new entry to the history\n",
        "    performance_history.append(entry)\n",
        "\n",
        "    # Display the updated experiment log\n",
        "    clear_output(wait=True)\n",
        "    df = pd.DataFrame(performance_history)\n",
        "    display(Markdown(df.to_markdown(index=False)))\n",
        "\n",
        "def remove_last_entry():\n",
        "    \"\"\"Remove the last entry from the experiment log.\"\"\"\n",
        "    global config_history, version_counter\n",
        "    if config_history:\n",
        "        config_history.pop()\n",
        "        version_counter -= 1\n",
        "        clear_output(wait=True)\n",
        "        df = pd.DataFrame(config_history)\n",
        "        display(Markdown(df.to_markdown(index=False)))\n",
        "\n",
        "def clear_table():\n",
        "    \"\"\"Clear the experiment log table.\"\"\"\n",
        "    global config_history, version_counter\n",
        "    config_history = []\n",
        "    version_counter = 0\n",
        "    clear_output(wait=True)\n",
        "\n",
        "def clear_performance_table():\n",
        "    \"\"\"Clear the performance log table.\"\"\"\n",
        "    global performance_history\n",
        "    performance_history = []\n",
        "    clear_output(wait=True)\n",
        "\n",
        "def print_table_for_github():\n",
        "    \"\"\"Print the experiment log table in markdown format for GitHub.\"\"\"\n",
        "    df = pd.DataFrame(config_history)\n",
        "    print(df.to_markdown(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 258,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Removes the last entry entered; only if I mean to\n",
        "can_remove = False\n",
        "\n",
        "if can_remove:\n",
        "    remove_last_entry()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clears the whole table, only if I mean to\n",
        "can_clear = False\n",
        "\n",
        "if can_clear:\n",
        "    clear_table()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 273,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Prints the table in markdown format\n",
        "print_table_for_github()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 261,
      "metadata": {
        "id": "GvXPdjPCBxOd"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "|   version | timestamp           | DEFAULT_ENV_NAME   |   MEAN_REWARD_BOUND |   GAMMA |   BATCH_SIZE |   REPLAY_SIZE |   LEARNING_RATE |   SYNC_TARGET_FRAMES |   REPLAY_START_SIZE |   SAVE_EPSILON |   EPSILON_DECAY_LAST_FRAME |   EPSILON_START |   EPSILON_FINAL | note         |\n",
              "|----------:|:--------------------|:-------------------|--------------------:|--------:|-------------:|--------------:|----------------:|---------------------:|--------------------:|---------------:|---------------------------:|----------------:|----------------:|:-------------|\n",
              "|         0 | 2025-11-12 08:17:29 | ALE/Pong-v5        |                  19 |    0.99 |           32 |         10000 |          0.0001 |                 1000 |               10000 |            0.5 |                     150000 |               1 |            0.01 | Baseline run |\n",
              "|         1 | 2025-11-12 08:23:33 | ALE/Pong-v5        |                   5 |    0.99 |           32 |         10000 |          0.0001 |                  500 |                1000 |            0.5 |                      10000 |               1 |            0.01 | Quick run    |\n",
              "|         2 | 2025-11-12 08:23:40 | ALE/Pong-v5        |                  19 |    0.99 |           32 |         10000 |          0.0001 |                 1000 |               10000 |            0.5 |                     150000 |               1 |            0.01 | Baseline run |\n",
              "|         3 | 2025-11-12 08:50:55 | ALE/Pong-v5        |                   5 |    0.99 |           32 |         10000 |          0.0001 |                  500 |                1000 |            0.5 |                      10000 |               1 |            0.01 | Quick run    |\n",
              "|         4 | 2025-11-12 10:25:28 | ALE/Pong-v5        |                  19 |    0.99 |           32 |         10000 |          0.0001 |                 1000 |               10000 |            0.5 |                     150000 |               1 |            0.01 | Baseline run |\n",
              "|         5 | 2025-11-12 10:25:28 | ALE/Pong-v5        |                   5 |    0.99 |           32 |         10000 |          0.0001 |                  500 |                1000 |            0.5 |                      10000 |               1 |            0.01 | Quick run    |\n",
              "|         6 | 2025-11-12 10:26:12 | ALE/Pong-v5        |                  19 |    0.99 |           32 |         10000 |          0.0001 |                 1000 |               10000 |            0.5 |                     150000 |               1 |            0.01 | Baseline run |\n",
              "|         7 | 2025-11-12 10:26:12 | ALE/Pong-v5        |                   5 |    0.99 |           32 |         10000 |          0.0001 |                  500 |                1000 |            0.5 |                      10000 |               1 |            0.01 | Quick run    |\n",
              "|         8 | 2025-11-12 11:02:14 | ALE/Pong-v5        |                  19 |    0.99 |           32 |         10000 |          0.0001 |                 1000 |               10000 |            0.5 |                     150000 |               1 |            0.01 | Baseline run |"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Base Configuration\n",
        "config = {\n",
        "    \"DEFAULT_ENV_NAME\": \"ALE/Pong-v5\",\n",
        "    \"MEAN_REWARD_BOUND\": 19,\n",
        "\n",
        "    \"GAMMA\": 0.99,\n",
        "    \"BATCH_SIZE\": 32,\n",
        "    \"REPLAY_SIZE\": 10000,\n",
        "    \"LEARNING_RATE\": 1e-4,\n",
        "    \"SYNC_TARGET_FRAMES\": 1000,\n",
        "    \"REPLAY_START_SIZE\": 10000,\n",
        "\n",
        "    \"SAVE_EPSILON\": 0.5,  # Only save if at least this much better\n",
        "    \"EPSILON_DECAY_LAST_FRAME\": 150000,\n",
        "    \"EPSILON_START\": 1.0,\n",
        "    \"EPSILON_FINAL\": 0.01\n",
        "}\n",
        "\n",
        "# Log automatically when you rerun this cell\n",
        "log_experiment(config, note=\"Baseline run\")\n",
        "\n",
        "# Tuple of tensors returned from a sampled minibatch in replay buffer\n",
        "State = np.ndarray\n",
        "Action = int\n",
        "BatchTensors = tt.Tuple[\n",
        "    torch.ByteTensor,           # current state\n",
        "    torch.LongTensor,           # actions\n",
        "    torch.Tensor,               # rewards\n",
        "    torch.BoolTensor,           # done || trunc\n",
        "    torch.ByteTensor            # next state\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 262,
      "metadata": {
        "id": "3FHvHNMrR9Sk"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "|   version | timestamp           | DEFAULT_ENV_NAME   |   MEAN_REWARD_BOUND |   GAMMA |   BATCH_SIZE |   REPLAY_SIZE |   LEARNING_RATE |   SYNC_TARGET_FRAMES |   REPLAY_START_SIZE |   SAVE_EPSILON |   EPSILON_DECAY_LAST_FRAME |   EPSILON_START |   EPSILON_FINAL | note         |\n",
              "|----------:|:--------------------|:-------------------|--------------------:|--------:|-------------:|--------------:|----------------:|---------------------:|--------------------:|---------------:|---------------------------:|----------------:|----------------:|:-------------|\n",
              "|         0 | 2025-11-12 08:17:29 | ALE/Pong-v5        |                  19 |    0.99 |           32 |         10000 |          0.0001 |                 1000 |               10000 |            0.5 |                     150000 |               1 |            0.01 | Baseline run |\n",
              "|         1 | 2025-11-12 08:23:33 | ALE/Pong-v5        |                   5 |    0.99 |           32 |         10000 |          0.0001 |                  500 |                1000 |            0.5 |                      10000 |               1 |            0.01 | Quick run    |\n",
              "|         2 | 2025-11-12 08:23:40 | ALE/Pong-v5        |                  19 |    0.99 |           32 |         10000 |          0.0001 |                 1000 |               10000 |            0.5 |                     150000 |               1 |            0.01 | Baseline run |\n",
              "|         3 | 2025-11-12 08:50:55 | ALE/Pong-v5        |                   5 |    0.99 |           32 |         10000 |          0.0001 |                  500 |                1000 |            0.5 |                      10000 |               1 |            0.01 | Quick run    |\n",
              "|         4 | 2025-11-12 10:25:28 | ALE/Pong-v5        |                  19 |    0.99 |           32 |         10000 |          0.0001 |                 1000 |               10000 |            0.5 |                     150000 |               1 |            0.01 | Baseline run |\n",
              "|         5 | 2025-11-12 10:25:28 | ALE/Pong-v5        |                   5 |    0.99 |           32 |         10000 |          0.0001 |                  500 |                1000 |            0.5 |                      10000 |               1 |            0.01 | Quick run    |\n",
              "|         6 | 2025-11-12 10:26:12 | ALE/Pong-v5        |                  19 |    0.99 |           32 |         10000 |          0.0001 |                 1000 |               10000 |            0.5 |                     150000 |               1 |            0.01 | Baseline run |\n",
              "|         7 | 2025-11-12 10:26:12 | ALE/Pong-v5        |                   5 |    0.99 |           32 |         10000 |          0.0001 |                  500 |                1000 |            0.5 |                      10000 |               1 |            0.01 | Quick run    |\n",
              "|         8 | 2025-11-12 11:02:14 | ALE/Pong-v5        |                  19 |    0.99 |           32 |         10000 |          0.0001 |                 1000 |               10000 |            0.5 |                     150000 |               1 |            0.01 | Baseline run |\n",
              "|         9 | 2025-11-12 11:02:15 | ALE/Pong-v5        |                   5 |    0.99 |           32 |         10000 |          0.0001 |                  500 |                1000 |            0.5 |                      10000 |               1 |            0.01 | Quick run    |"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# ‚öôÔ∏è Fast Training Config for Quick Test Run\n",
        "config[\"MEAN_REWARD_BOUND\"] = 5\n",
        "config[\"REPLAY_START_SIZE\"] = 1000\n",
        "config[\"EPSILON_DECAY_LAST_FRAME\"] = 10000\n",
        "config[\"SYNC_TARGET_FRAMES\"] = 500\n",
        "\n",
        "# config[\"REPLAY_SIZE\"] = 5000  # optional\n",
        "# config[\"BATCH_SIZE\"] = 16     # optional\n",
        "\n",
        "# Log automatically when you rerun this cell\n",
        "log_experiment(config, note=\"Quick run\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 263,
      "metadata": {
        "id": "MkILCuT2OhBV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Define directories\n",
        "save_dir_local = \"saved_models\"\n",
        "\n",
        "# Create both directories if they don't exist\n",
        "os.makedirs(save_dir_local, exist_ok=True)\n",
        "\n",
        "# Safe model filename\n",
        "env_name = config[\"DEFAULT_ENV_NAME\"]\n",
        "safe_env_name = env_name.replace(\"/\", \"_\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 264,
      "metadata": {
        "id": "uW4Bo-mcB6rc"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class Experience:\n",
        "    state: State\n",
        "    action: Action\n",
        "    reward: float\n",
        "    done_trunc: bool\n",
        "    new_state: State\n",
        "\n",
        "\n",
        "class ExperienceBuffer:\n",
        "    def __init__(self, capacity: int):\n",
        "        self.buffer = collections.deque(maxlen=capacity)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def append(self, experience: Experience):\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    def sample(self, batch_size: int) -> tt.List[Experience]:\n",
        "        indices = np.random.choice(len(self), batch_size, replace=False)\n",
        "        return [self.buffer[idx] for idx in indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 265,
      "metadata": {
        "id": "irJb4V32B-R8"
      },
      "outputs": [],
      "source": [
        "class Agent:\n",
        "    def __init__(self, env: gym.Env, exp_buffer: ExperienceBuffer):\n",
        "        self.env = env\n",
        "        self.exp_buffer = exp_buffer\n",
        "        self.state: tt.Optional[np.ndarray] = None\n",
        "        self._reset()\n",
        "\n",
        "    def _reset(self):\n",
        "        self.state, _ = self.env.reset()\n",
        "        self.total_reward = 0.0\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def play_step(self, net: DQN, device: torch.device,\n",
        "                  epsilon: float = 0.0) -> tt.Optional[float]:\n",
        "        done_reward = None\n",
        "\n",
        "        if np.random.random() < epsilon:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            state_v = torch.as_tensor(self.state).to(device)\n",
        "            state_v.unsqueeze_(0)\n",
        "            q_vals_v = net(state_v)\n",
        "            _, act_v = torch.max(q_vals_v, dim=1)\n",
        "            action = int(act_v.item())\n",
        "\n",
        "        # do step in the environment\n",
        "        new_state, reward, is_done, is_tr, _ = self.env.step(action)\n",
        "        self.total_reward += reward\n",
        "\n",
        "        exp = Experience(\n",
        "            state=self.state, action=action, reward=float(reward),\n",
        "            done_trunc=is_done or is_tr, new_state=new_state\n",
        "        )\n",
        "        self.exp_buffer.append(exp)\n",
        "        self.state = new_state\n",
        "        if is_done or is_tr:\n",
        "            done_reward = self.total_reward\n",
        "            self._reset()\n",
        "        return done_reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 266,
      "metadata": {
        "id": "vHXmNr_wCBJm"
      },
      "outputs": [],
      "source": [
        "def batch_to_tensors(batch: tt.List[Experience], device: torch.device) -> BatchTensors:\n",
        "    states, actions, rewards, dones, new_state = [], [], [], [], []\n",
        "    for e in batch:\n",
        "        states.append(e.state)\n",
        "        actions.append(e.action)\n",
        "        rewards.append(e.reward)\n",
        "        dones.append(e.done_trunc)\n",
        "        new_state.append(e.new_state)\n",
        "    states_t = torch.as_tensor(np.asarray(states))\n",
        "    actions_t = torch.LongTensor(actions)\n",
        "    rewards_t = torch.FloatTensor(rewards)\n",
        "    dones_t = torch.BoolTensor(dones)\n",
        "    new_states_t = torch.as_tensor(np.asarray(new_state))\n",
        "    return states_t.to(device), actions_t.to(device), rewards_t.to(device), \\\n",
        "           dones_t.to(device),  new_states_t.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 267,
      "metadata": {
        "id": "-dbh0431CEXO"
      },
      "outputs": [],
      "source": [
        "def calc_loss(batch: tt.List[Experience], net: DQN, tgt_net: DQN,\n",
        "              device: torch.device) -> torch.Tensor:\n",
        "    states_t, actions_t, rewards_t, dones_t, new_states_t = batch_to_tensors(batch, device)\n",
        "\n",
        "    state_action_values = net(states_t).gather(\n",
        "        1, actions_t.unsqueeze(-1)\n",
        "    ).squeeze(-1)\n",
        "\n",
        "    # Decide whether to use Double DQN\n",
        "    USE_DOUBLE_DQN = False\n",
        "\n",
        "    # Compute the expected Q values\n",
        "    with torch.no_grad():\n",
        "        if USE_DOUBLE_DQN: # Double DQN\n",
        "            next_actions = net(new_states_t).argmax(dim=1)\n",
        "            next_state_values = tgt_net(new_states_t).gather(1, next_actions.unsqueeze(-1)).squeeze(-1)\n",
        "        else: # DQN\n",
        "            next_state_values = tgt_net(new_states_t).max(1)[0]\n",
        "\n",
        "        next_state_values[dones_t] = 0.0\n",
        "\n",
        "    expected_state_action_values = rewards_t + config[\"GAMMA\"] * next_state_values; \n",
        "\n",
        "    return nn.MSELoss()(state_action_values, expected_state_action_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 268,
      "metadata": {},
      "outputs": [],
      "source": [
        "def record_policy_video(env_name, model, device, video_filename=\"policy_rollout.mp4\", max_steps=2000):\n",
        "    \"\"\"Run one greedy episode using the given model and save a video.\"\"\"\n",
        "    # Use your make_env to preserve preprocessing\n",
        "    video_env = make_env(env_name, render_mode=\"rgb_array\")\n",
        "    state, _ = video_env.reset(seed=42)\n",
        "    frames, total_reward = [], 0.0\n",
        "\n",
        "    for step in range(max_steps):\n",
        "        # Capture a rendered RGB frame (already handled by render_mode)\n",
        "        frame = video_env.render()\n",
        "        if isinstance(frame, np.ndarray):\n",
        "            frames.append(frame)\n",
        "\n",
        "        # Greedy action\n",
        "        state_v = torch.as_tensor(state).unsqueeze(0).to(device)\n",
        "        q_vals_v = model(state_v)\n",
        "        _, act_v = torch.max(q_vals_v, dim=1)\n",
        "        action = int(act_v.item())\n",
        "\n",
        "        state, reward, terminated, truncated, _ = video_env.step(action)\n",
        "        total_reward += reward\n",
        "        if terminated or truncated:\n",
        "            frames.append(video_env.render())\n",
        "            break\n",
        "\n",
        "    video_env.close()\n",
        "\n",
        "    # Save video\n",
        "    if frames:\n",
        "        os.makedirs(os.path.dirname(video_filename), exist_ok=True)\n",
        "\n",
        "        # üîπ Pad frames so height is divisible by 16 (prevents FFMPEG warnings)\n",
        "        padded_frames = []\n",
        "        for f in frames:\n",
        "            h, w = f.shape[:2]\n",
        "            new_h = (h + 15) // 16 * 16  # round up to nearest multiple of 16\n",
        "            if new_h != h:\n",
        "                pad = np.zeros((new_h - h, w, 3), dtype=f.dtype)\n",
        "                f = np.vstack([f, pad])\n",
        "            padded_frames.append(f)\n",
        "\n",
        "        # üîπ Write padded frames quietly\n",
        "        with imageio.get_writer(video_filename, fps=30) as writer:\n",
        "            for f in padded_frames:\n",
        "                writer.append_data(f)\n",
        "\n",
        "        print(f\"üé¨ Saved gameplay video: {video_filename} | Total reward: {total_reward:.2f}\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è No frames captured ‚Äî render_mode may not be supported.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 271,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8gj-5woCXEB",
        "outputId": "afd1308e-a016-4b02-b944-4321357467fd"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "|   Frame Index | model_name   |   Finished Games |   Reward |   Epsilon |   Speed (f/s) |   Elapsed Time (min) | timestamp           |\n",
              "|--------------:|:-------------|-----------------:|---------:|----------:|--------------:|---------------------:|:--------------------|\n",
              "|           218 | DQN          |                1 |   -20    |    0.9782 |      269.29   |            0.0134627 | 2025-11-12 11:05:17 |\n",
              "|         56135 | DQN          |              181 |   -19.47 |    0.01   |       32.3526 |           29.5246    | 2025-11-12 11:34:48 |\n",
              "|         62737 | DQN          |              193 |   -18.94 |    0.01   |       31.6536 |           33.0653    | 2025-11-12 11:38:20 |\n",
              "|         69015 | DQN          |              204 |   -18.43 |    0.01   |       30.8131 |           36.437     | 2025-11-12 11:41:42 |\n",
              "|         76055 | DQN          |              216 |   -17.9  |    0.01   |       31.0258 |           40.3123    | 2025-11-12 11:45:35 |\n",
              "|         82799 | DQN          |              227 |   -17.38 |    0.01   |       30.2709 |           44.1065    | 2025-11-12 11:49:23 |"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model saved to:\n",
            " - Local:        saved_models\\ALE_Pong-v5-best_-17-20251112-1149-test_epsdec10000_rs1000_sync500.dat\n",
            "Best reward updated -17.900 -> -17.380\n",
            "Creating environment ALE/Pong-v5\n",
            "üé¨ Saved gameplay video: videos/ALE_Pong-v5--17-20251112-1149.mp4 | Total reward: -10.00\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[271]\u001b[39m\u001b[32m, line 103\u001b[39m\n\u001b[32m    101\u001b[39m     loss_t = calc_loss(batch, net, tgt_net, device)\n\u001b[32m    102\u001b[39m     loss_t.backward()\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m     \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    104\u001b[39m env.close()\n\u001b[32m    105\u001b[39m writer.close()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\optim\\optimizer.py:517\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    512\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    513\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    514\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    515\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m517\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    520\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\optim\\optimizer.py:82\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     80\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     81\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     84\u001b[39m     torch._dynamo.graph_break()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\optim\\adam.py:247\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    235\u001b[39m     beta1, beta2 = group[\u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    237\u001b[39m     has_complex = \u001b[38;5;28mself\u001b[39m._init_group(\n\u001b[32m    238\u001b[39m         group,\n\u001b[32m    239\u001b[39m         params_with_grad,\n\u001b[32m   (...)\u001b[39m\u001b[32m    244\u001b[39m         state_steps,\n\u001b[32m    245\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mamsgrad\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaximize\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mforeach\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcapturable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdifferentiable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfused\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgrad_scale\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdecoupled_weight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\optim\\optimizer.py:150\u001b[39m, in \u001b[36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(*args, **kwargs)\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\optim\\adam.py:953\u001b[39m, in \u001b[36madam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[39m\n\u001b[32m    950\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    951\u001b[39m     func = _single_tensor_adam\n\u001b[32m--> \u001b[39m\u001b[32m953\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\optim\\adam.py:422\u001b[39m, in \u001b[36m_single_tensor_adam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[39m\n\u001b[32m    419\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    420\u001b[39m             grad = grad.add(param, alpha=weight_decay)\n\u001b[32m--> \u001b[39m\u001b[32m422\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    423\u001b[39m     grad = torch.view_as_real(grad)\n\u001b[32m    424\u001b[39m     exp_avg = torch.view_as_real(exp_avg)\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# Training setup\n",
        "model_comment = f\"test_epsdec{config[\"EPSILON_DECAY_LAST_FRAME\"]}_rs{config[\"REPLAY_START_SIZE\"]}_sync{config[\"SYNC_TARGET_FRAMES\"]}\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Environment and networks\n",
        "env = make_env(env_name)\n",
        "net = DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
        "tgt_net = DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
        "writer = SummaryWriter(comment=f\"-{env_name}-{model_comment}\")\n",
        "print(net)\n",
        "\n",
        "# Experience replay buffer and agent\n",
        "buffer = ExperienceBuffer(config[\"REPLAY_SIZE\"])\n",
        "agent = Agent(env, buffer)\n",
        "epsilon = config[\"EPSILON_START\"]\n",
        "\n",
        "# Optimizer setup\n",
        "optimizer = optim.Adam(net.parameters(), lr=config[\"LEARNING_RATE\"])\n",
        "total_rewards = []\n",
        "frame_idx = 0\n",
        "ts_frame = 0\n",
        "ts = time.time()\n",
        "best_m_reward = None\n",
        "\n",
        "USE_DOUBLE_DQN = False  # Set to True to use Double DQN\n",
        "\n",
        "if USE_DOUBLE_DQN:\n",
        "    model_name = \"Double DQN\"\n",
        "else:\n",
        "    model_name = \"DQN\"\n",
        "\n",
        "performance_metric = {\n",
        "    \"Finished Games\": 0,\n",
        "    \"Reward\": 0.0,\n",
        "    \"Epsilon\": 0.0,\n",
        "    \"Speed (f/s)\": 0.0,\n",
        "    \"Elapsed Time (min)\": 0.0\n",
        "}\n",
        "\n",
        "# Training loop\n",
        "start_time = time.time()\n",
        "while True:\n",
        "    frame_idx += 1\n",
        "    epsilon = max(config[\"EPSILON_FINAL\"], config[\"EPSILON_START\"] - frame_idx / config[\"EPSILON_DECAY_LAST_FRAME\"])\n",
        "\n",
        "    reward = agent.play_step(net, device, epsilon)\n",
        "    if reward is not None:\n",
        "        total_rewards.append(reward)\n",
        "        speed = (frame_idx - ts_frame) / (time.time() - ts)\n",
        "        elapsed = time.time() - start_time  # in seconds\n",
        "        ts_frame = frame_idx\n",
        "        ts = time.time()\n",
        "        m_reward = np.mean(total_rewards[-100:])\n",
        "        #  print(f\"{frame_idx}: done {len(total_rewards)} games, reward {m_reward:.3f}, \"\n",
        "        #      f\"eps {epsilon:.2f}, speed {speed:.2f} f/s, time {elapsed/60:.1f} min\")\n",
        "        writer.add_scalar(\"epsilon\", epsilon, frame_idx)\n",
        "        writer.add_scalar(\"speed\", speed, frame_idx)\n",
        "        writer.add_scalar(\"reward_100\", m_reward, frame_idx)\n",
        "        writer.add_scalar(\"reward\", reward, frame_idx)\n",
        "        if best_m_reward is None or m_reward > best_m_reward + config[\"SAVE_EPSILON\"]:\n",
        "            print(f\"{frame_idx}: done {len(total_rewards)} games, reward {m_reward:.3f}, \"\n",
        "                f\"eps {epsilon:.2f}, speed {speed:.2f} f/s, time {elapsed/60:.1f} min\")\n",
        "            \n",
        "            # Update performance metrics\n",
        "            performance_metric[\"Finished Games\"] = len(total_rewards)\n",
        "            performance_metric[\"Reward\"] = m_reward\n",
        "            performance_metric[\"Epsilon\"] = epsilon\n",
        "            performance_metric[\"Speed (f/s)\"] = speed\n",
        "            performance_metric[\"Elapsed Time (min)\"] = elapsed / 60.0\n",
        "\n",
        "            # Log model performance\n",
        "            log_model_performance(frame_idx, model_name, performance_metric)\n",
        "\n",
        "            # Save the model\n",
        "            timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
        "            model_filename = f\"{safe_env_name}-best_{int(m_reward)}-{timestamp}-{model_comment}.dat\"\n",
        "            model_path_local = os.path.join(save_dir_local, model_filename)\n",
        "\n",
        "            torch.save(net.state_dict(), model_path_local)\n",
        "\n",
        "            print(f\"Model saved to:\\n - Local:        {model_path_local}\")\n",
        "            if best_m_reward is not None:\n",
        "                print(f\"Best reward updated {best_m_reward:.3f} -> {m_reward:.3f}\")\n",
        "            best_m_reward = m_reward\n",
        "\n",
        "            # üé• Automatically record a short greedy rollout video\n",
        "            video_filename = f\"videos/{safe_env_name}-{int(m_reward)}-{timestamp}.mp4\"\n",
        "            os.makedirs(\"videos\", exist_ok=True)\n",
        "            record_policy_video(env_name, net, device, video_filename)\n",
        "        if m_reward > config[\"MEAN_REWARD_BOUND\"]:\n",
        "            print(\"Solved in %d frames!\" % frame_idx)\n",
        "            break\n",
        "    if len(buffer) < config[\"REPLAY_START_SIZE\"]:\n",
        "        continue\n",
        "    if frame_idx % config[\"SYNC_TARGET_FRAMES\"] == 0:\n",
        "        tgt_net.load_state_dict(net.state_dict())\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    batch = buffer.sample(config[\"BATCH_SIZE\"])\n",
        "    loss_t = calc_loss(batch, net, tgt_net, device)\n",
        "    loss_t.backward()\n",
        "    optimizer.step()\n",
        "env.close()\n",
        "writer.close()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
